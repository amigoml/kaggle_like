{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Вам предстоит построить модель, которая в зависимости цены, заголовка, категории и времени размещения объявления предсказывает количество просмотров объявления до конца следующего дня с момента подачи.\n",
    "\n",
    "\n",
    "Описание данных\n",
    "start_time\tВремя подачи объявления (МСК)\n",
    "title\tЗаголовок объявления\n",
    "price\tЦена\n",
    "item_id\tИдентификатор объявления\n",
    "owner_type\tТип владельца объявления (Private – частный пользователь; Company – компания; Shop – владелец магазина на Avito)\n",
    "category\tКатегория объявления (Транспорт, недвижимость и т.д.)\n",
    "subcategory\tПодкатегория объявления\n",
    "param1, param2, param3\tПараметры объявления*. Отсортированы по нахождению в дереве категорий Авито. Можно посмотреть на сайте\n",
    "region\tРегион размещения объявления\n",
    "item_views\tЦелевая переменная, кол-во просмотров объявления\n",
    "\n",
    "* На примере подкатегории «Детская одежда и обувь»\n",
    "• param1 = Вид одежды = «Для мальчиков» \n",
    "• param2 = Предмет одежды = “Верхняя одежда» \n",
    "• param3 = Размер = «50-56 cм (0-2 мес)»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('train.csv', sep=';', nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>﻿id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>item_id</th>\n",
       "      <th>owner_type</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>param1</th>\n",
       "      <th>param2</th>\n",
       "      <th>param3</th>\n",
       "      <th>region</th>\n",
       "      <th>item_views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-12-27 10:38:04</td>\n",
       "      <td>Сандали фирмы Crocs</td>\n",
       "      <td>800</td>\n",
       "      <td>1301822498390501359</td>\n",
       "      <td>Private</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Для мальчиков</td>\n",
       "      <td>Обувь</td>\n",
       "      <td>&gt; 36</td>\n",
       "      <td>Москва</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-12-27 15:23:55</td>\n",
       "      <td>Бутсы футбольные Reebok</td>\n",
       "      <td>2000</td>\n",
       "      <td>4439620035274845039</td>\n",
       "      <td>Private</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Для мальчиков</td>\n",
       "      <td>Обувь</td>\n",
       "      <td>&gt; 36</td>\n",
       "      <td>Омская область</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-28 19:34:15</td>\n",
       "      <td>Nike hypervenom Бутсы</td>\n",
       "      <td>600</td>\n",
       "      <td>4860577743813309218</td>\n",
       "      <td>Private</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Для мальчиков</td>\n",
       "      <td>Обувь</td>\n",
       "      <td>&gt; 36</td>\n",
       "      <td>Санкт-Петербург</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-12-26 10:26:02</td>\n",
       "      <td>Сапоги</td>\n",
       "      <td>150</td>\n",
       "      <td>3492530336858889466</td>\n",
       "      <td>Private</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Для мальчиков</td>\n",
       "      <td>Обувь</td>\n",
       "      <td>&gt; 36</td>\n",
       "      <td>Тульская область</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-12-26 17:09:19</td>\n",
       "      <td>Кеды 38</td>\n",
       "      <td>500</td>\n",
       "      <td>3559049054931858928</td>\n",
       "      <td>Private</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Для мальчиков</td>\n",
       "      <td>Обувь</td>\n",
       "      <td>&gt; 36</td>\n",
       "      <td>Самарская область</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ﻿id           start_time                    title  price  \\\n",
       "0    0  2016-12-27 10:38:04      Сандали фирмы Crocs    800   \n",
       "1    1  2016-12-27 15:23:55  Бутсы футбольные Reebok   2000   \n",
       "2    2  2016-12-28 19:34:15    Nike hypervenom Бутсы    600   \n",
       "3    3  2016-12-26 10:26:02                   Сапоги    150   \n",
       "4    4  2016-12-26 17:09:19                  Кеды 38    500   \n",
       "\n",
       "               item_id owner_type     category             subcategory  \\\n",
       "0  1301822498390501359    Private  Личные вещи  Детская одежда и обувь   \n",
       "1  4439620035274845039    Private  Личные вещи  Детская одежда и обувь   \n",
       "2  4860577743813309218    Private  Личные вещи  Детская одежда и обувь   \n",
       "3  3492530336858889466    Private  Личные вещи  Детская одежда и обувь   \n",
       "4  3559049054931858928    Private  Личные вещи  Детская одежда и обувь   \n",
       "\n",
       "          param1 param2 param3             region  item_views  \n",
       "0  Для мальчиков  Обувь   > 36             Москва          27  \n",
       "1  Для мальчиков  Обувь   > 36     Омская область           9  \n",
       "2  Для мальчиков  Обувь   > 36    Санкт-Петербург         105  \n",
       "3  Для мальчиков  Обувь   > 36   Тульская область          28  \n",
       "4  Для мальчиков  Обувь   > 36  Самарская область           9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mount/neuro-t01-ssd/home/amir/my_env/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "from  datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.sparse import csc_matrix, vstack, hstack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import SnowballStemmer\n",
    "import nltk.corpus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import random\n",
    "random.seed(666)\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_____\n",
    "\n",
    "##### Plots"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.plot(data_train.item_views, data_train.item_id, '.b', alpha=0.4);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "tmp = data_train.region.astype('category')\n",
    "lbs = tmp.cat.rename_categories(range(tmp.cat.categories.values.__len__()))\n",
    "plt.plot(data_train.item_views, lbs.values.labels, '.r', alpha=0.3);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.plot(data_train.item_views, data_train.price, '.b', alpha=0.3);\n",
    "plt.ylim([1,50000000]);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.plot(data_train.item_views,\n",
    "data_train.start_time.apply(lambda x: x.hour).values, '.b', alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___________\n",
    "\n",
    "\n",
    "#### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _fillNa(dt, fill_na_train, is_train=True):\n",
    "    \"\"\" 1\n",
    "    Fillig nan values for next columns:\n",
    "    'title', 'param1', 'param2', 'param3'.\n",
    "    \"\"\"\n",
    "    if not is_train and (fill_na_train.__len__() == 0):\n",
    "        raise Exception('Run on train data before!')\n",
    "    \n",
    "    if is_train:\n",
    "        fill_na_train = {}\n",
    "        for col in ['title', 'param1', 'param2', 'param3']:\n",
    "            fill_na_train[col] = 'nan'\n",
    "    dt.fillna(fill_na_train, inplace=True)\n",
    "    return fill_na_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _working_with_time(dt):\n",
    "    \"\"\" 2\n",
    "    Parse datetime column. Extract hours (slided by -3.5), \n",
    "    week_day, relative time with minute (from 0 to 1).\n",
    "    \"\"\"\n",
    "    dt.start_time = dt.start_time.astype(datetime)\n",
    "    dt['rel_times'] = np.array(list(map(lambda dt: (dt.hour*60 + dt.minute)/(24.*60), \n",
    "                                                dt.start_time.values))).astype(float)\n",
    "    dt['week_day'] = np.array(list(map( lambda x: x.isoweekday() ,\n",
    "                                                dt.start_time.values))).astype(int)\n",
    "    dt['slided_hours'] = np.array(list(map( lambda x: ((x.hour - 3.5) % 24) * 1. / 24.,\n",
    "                                                dt.start_time.values))).astype(float)\n",
    "    dt.drop(['start_time'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_new_str_features(dt):\n",
    "    \"\"\" 3\n",
    "    Create new features for gender, mar_status,\n",
    "    liv_region and other string columns. \n",
    "    ------------\n",
    "    Return\n",
    "    \n",
    "    str_cols : list of str\n",
    "        Columns for encoding and deleting.\n",
    "    \"\"\"\n",
    "#     #one_hot_encoding\n",
    "#     dt['owner_type_Private'] = (dt.owner_type=='Private').astype(int)\n",
    "#     dt['owner_type_Company'] = (dt.owner_type=='Company').astype(int)\n",
    "#     dt['owner_type_Shop'] = (dt.owner_type=='Shop').astype(int)\n",
    "\n",
    "    # конкатенация строк\n",
    "    dt['all_params'] = dt.param1 + ' ' + dt.param2 + ' ' + dt.param3\n",
    "    dt['params_12'] = dt.param1 + ' ' + dt.param2\n",
    "    dt['params_23'] = dt.param2 + ' ' + dt.param3\n",
    "    dt['category_and_owner_type'] = dt.owner_type + ' ' + dt.category\n",
    "    dt['category_and_region'] = dt.region + ' ' + dt.category\n",
    "    dt['owner_type_and_region'] = dt.owner_type + ' ' + dt.category\n",
    "    \n",
    "    str_cols = ['owner_type', \n",
    "                'param1', 'param2', 'param3',\n",
    "                'region', 'category' , 'subcategory', \n",
    "                'params_12', 'params_23', 'all_params',\n",
    "                'category_and_owner_type',\n",
    "                'category_and_region', 'owner_type_and_region'\n",
    "               ]\n",
    "    return str_cols"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def _onehot_encoder(dt, str_cols, onehot_Encoders, is_train=True):\n",
    "    \"\"\" 4\n",
    "    Counter Encoder. \n",
    "    Set to the categories from 'str_cols' columns some \n",
    "    numbers - frequencies in train. \n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        onehot_Encoders = {col: dt[col].unique() for col in str_cols}\n",
    "    for column in str_cols:\n",
    "        for value in onehot_Encoders[column]:\n",
    "            dt[column + '_' + value] = (dt[column]==value).astype(int)\n",
    "    return onehot_Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _counter_encoder(dt, str_cols, counter_Encoders, is_train=True):\n",
    "    \"\"\" 4\n",
    "    Counter Encoder. \n",
    "    Set to the categories from 'str_cols' columns some numbers \n",
    "    (frequencies in train). \n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        counter_Encoders = {col:dt[col].value_counts().to_dict()\n",
    "                                            for col in str_cols}\n",
    "    for column in str_cols:\n",
    "        dt[column+'_enc_by_count'] = dt[column].apply(lambda x: \n",
    "                                counter_Encoders[column].get(x, 0)) \n",
    "                                # TODO ровнее бы\n",
    "    return counter_Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _target_encoder(dt, cols_for_encoding, targ_encoders, is_train=True, target_mean = 60.1):\n",
    "    \"\"\" 5\n",
    "    Encode cat values by the mean in target.\n",
    "    \n",
    "    Params\n",
    "    dt : DataFrame\n",
    "        Data.\n",
    "    cols_for_encoding : list\n",
    "        List of columns for encode.\n",
    "    targ_encoders : dict\n",
    "        Encoder for test dataset.\n",
    "    is_train : bool\n",
    "        Flag for train/test.\n",
    "    \n",
    "    ------\n",
    "    Return\n",
    "    targ_encoders : dict\n",
    "        Values of mean target for each category \n",
    "        in columns from cols_for_encoding.\n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        targ_encoders = {}\n",
    "        targ_means = {}\n",
    "        for col in cols_for_encoding:\n",
    "            targ_means[col] = dt.groupby(col).item_views.\\\n",
    "                            mean().sort_values().index.values\n",
    "            \n",
    "        for col in cols_for_encoding:\n",
    "            targ_encoders[col] = {v:i for i,v in enumerate(targ_means[col])}\n",
    "            \n",
    "    for col in cols_for_encoding:\n",
    "        dt[col+'_by_mean_target'] = dt[col].apply(lambda x:\n",
    "                                    targ_encoders[col].get(x, target_mean))\n",
    "        \n",
    "    return targ_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working with title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/c/avito-prohibited-content/data \n",
    "\n",
    "stopwords= frozenset(word for word in nltk.corpus.stopwords.words(\"russian\") \\\n",
    "                     if word!=\"не\")  #(word.decode('utf-8')\n",
    "stemmer = SnowballStemmer('russian')\n",
    "engChars = [ord(char) for char in u\"cCyoOBaAKpPeE\"]\n",
    "rusChars = [ord(char) for char in u\"сСуоОВаАКрРеЕ\"]\n",
    "eng_rusTranslateTable = dict(zip(engChars, rusChars))\n",
    "rus_engTranslateTable = dict(zip(rusChars, engChars))\n",
    "\n",
    "\n",
    "def correctWord (w):\n",
    "    \"\"\" \n",
    "    Corrects word by replacing characters with\n",
    "    written similarly depending on which language the word. \n",
    "    Fraudsters use this technique to avoid \n",
    "    detection by anti-fraud algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(re.findall('[а-я]',w))>len(re.findall('[a-z]',w)):\n",
    "        return w.translate(eng_rusTranslateTable)\n",
    "    else:\n",
    "        return w.translate(rus_engTranslateTable)\n",
    "\n",
    "    \n",
    "def getWords(text, stemmRequired = False, correctWordRequired = False):\n",
    "    \"\"\" Splits the text into words, discards stop words and applies stemmer. \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str - initial string\n",
    "    stemmRequired : bool - flag whether stemming required\n",
    "    correctWordRequired : bool - flag whether correction of words required     \n",
    "    \"\"\"\n",
    "\n",
    "    cleanText = re.sub(u'[^a-zа-я]', ' ', text.lower()) ###'[^a-zа-я0-9]'\n",
    "    if correctWordRequired:\n",
    "        words = [correctWord(w) if not stemmRequired or re.search('[a-z]', w) \\\n",
    "                 else stemmer.stem(correctWord(w)) for w in cleanText.split() \\\n",
    "                 if len(w)>1 and w not in stopwords]\n",
    "    else:\n",
    "        words = [w if not stemmRequired or re.search(\"[a-z]\", w) else stemmer.stem(w) \\\n",
    "                 for w in cleanText.split() if len(w)>1 and w not in stopwords]\n",
    "    \n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ghbdt asd rgrg ptp'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(['ghbdt', 'asd', 'rgrg', 'ptp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _title_normalize(dtr, dts):\n",
    "    \"\"\" 2\n",
    "    # prepr title - stemming\n",
    "    \"\"\"\n",
    "    prepr_words = lambda title: ' '.join(getWords(title + ' qwe qwe ', \n",
    "                                                  stemmRequired=True,\n",
    "                                                  correctWordRequired=False))\n",
    "    dtr['title1'] = dtr.title.apply(prepr_words)\n",
    "    \n",
    "    dts['title1']= dts.title.apply(prepr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cv_pipline(dt_tr, dt_ts, cv=True):\n",
    "    \"\"\"\n",
    "    Pipline for cv. and for train and test data.\n",
    "    \"\"\"\n",
    "    col_not_used = ['item_id', 'id']\n",
    "    \n",
    "    data_train = dt_tr#.copy()\n",
    "    data_test = dt_ts#.copy()\n",
    "    y_tr = data_train.item_views\n",
    "    if cv:\n",
    "        y_ts = data_test.item_views\n",
    "    print('filling na')\n",
    "    \n",
    "    # filling na\n",
    "    fill_na_train = _fillNa(data_train, {}, is_train=True)\n",
    "    _ = _fillNa(data_test, fill_na_train, is_train=False)\n",
    "    print('time processing')\n",
    "    \n",
    "    # time features\n",
    "    _working_with_time(data_train)\n",
    "    _working_with_time(data_test)\n",
    "    \n",
    "    print('str features')\n",
    "    # string features\n",
    "    str_cols = _get_new_str_features(data_train)\n",
    "    _ = _get_new_str_features(data_test)\n",
    "    \n",
    "    print('counter encoder')\n",
    "    # encoding string features\n",
    "    counter_Encoders = _counter_encoder(data_train,\n",
    "                                        str_cols, {}, is_train=True)\n",
    "    _ = _counter_encoder(data_test, str_cols,\n",
    "                         counter_Encoders, is_train=False)\n",
    "    \n",
    "    print ('target_mean_ecoder')\n",
    "    \n",
    "    # Very Caution! target_mean_ecncoder\n",
    "    cols_for_encoding = str_cols\n",
    "    targ_encoders = _target_encoder(data_train,\n",
    "                        cols_for_encoding, {},\n",
    "                        is_train=True,\n",
    "                        target_mean = int(data_train.item_views.mean()))\n",
    "    \n",
    "    _ = _target_encoder(data_test, cols_for_encoding,\n",
    "                        targ_encoders, is_train=False,\n",
    "                        target_mean = int(data_train.item_views.mean()))\n",
    "    \n",
    "    \n",
    "    print ('title_prepr')\n",
    "    \n",
    "    all_words = data_train.title.values\n",
    "    wordCounts = defaultdict(lambda: 0)\n",
    "    for item in all_words:\n",
    "        for word in getWords(item, stemmRequired=True, \n",
    "                             correctWordRequired=False):\n",
    "            wordCounts[word] += 1\n",
    "    top_words = np.array(sorted(wordCounts, key=wordCounts.get, \n",
    "                                reverse=True)[:11000])\n",
    "    \n",
    "    _title_normalize(data_train, data_test)\n",
    "    \n",
    "    print ('title_top')\n",
    "    \n",
    "    # map\n",
    "    \n",
    "    data_train.drop(str_cols, axis=1, inplace=True)\n",
    "    data_test.drop(str_cols, axis=1, inplace=True)\n",
    "    data_train.drop(col_not_used, axis=1, inplace=True)\n",
    "    data_test.drop(col_not_used, axis=1, inplace=True)\n",
    "    \n",
    "    some_vals1 = data_train.title1\n",
    "    some_vals2 = data_test.title1\n",
    "    \n",
    "    data_train.drop(['item_views', 'title', 'title1'], axis=1, inplace=True)\n",
    "    data_test.drop(['title', 'title1'], axis=1, inplace=True)\n",
    "    \n",
    "    data_tr_val = csc_matrix(data_train.values)\n",
    "    del data_train\n",
    "    data_ts_val = csc_matrix(data_test.values)\n",
    "    del data_test\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(vocabulary=top_words)\n",
    "    vals_tr = vectorizer.transform(some_vals1)\n",
    "    vals_ts = vectorizer.transform(some_vals2)\n",
    "        \n",
    "    tr_data = csc_matrix(hstack([data_tr_val, vals_tr]))\n",
    "    ts_data = csc_matrix(hstack([data_ts_val, vals_ts]))\n",
    "       \n",
    "    return tr_data, ts_data, np.log(y_tr.values+1.)\n",
    "\n",
    "    \n",
    "#     if cv:\n",
    "#         print ('run!')\n",
    "#         data_test.drop(['item_views', 'title', 'title1'], axis=1, inplace=True)\n",
    "#         return data_train, data_test, np.log(y_tr.values+1.), np.log(y_ts.values+1.)\n",
    "    \n",
    "#     else:\n",
    "#         #data_test.drop([ 'title', 'title1'], axis=1, inplace=True)\n",
    "#         return data_train, data_test, np.log(y_tr.values+1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Предсказываем сдвинутый логарифм np.log(y_tr.values+1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# локальная проверка на одном сплите  - коррелирует с лидербордом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling na\n",
      "time processing\n",
      "str features\n",
      "counter encoder\n",
      "target_mean_ecoder\n",
      "title_prepr\n",
      "title_top\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('train.csv', sep=';', parse_dates=['start_time'])\n",
    "data_test = pd.read_csv('test.csv', sep=';', parse_dates=['start_time'])\n",
    "dt_tr, dt_ts, y_tr = cv_pipline(data_train, data_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((423772, 11030), (418991, 11030))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_tr.shape, dt_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=846)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,j in cv.split(y_tr):\n",
    "    tr, ts = i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print ('run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 15,\n",
    "    'eta': 0.029,\n",
    "    'gamma' : 0,\n",
    "    'booster': 'gbtree',\n",
    "    'seed': 1,\n",
    "    'alpha': 2.0,\n",
    "    'lambda': 0.1,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'colsample_bylevel': 1.0,\n",
    "    'subsample': 0.95,\n",
    "    'min_child_weight': 1,\n",
    "    'silent': 1,\n",
    "    'nthread': 10,\n",
    "    'eval_metric':'rmse'\n",
    "}\n",
    "\n",
    "num_rounds = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (datetime.datetime.now())\n",
    "dtrain = xgb.DMatrix(dt_tr[tr], label=y_tr[tr])\n",
    "dtest = xgb.DMatrix(dt_tr[ts], label=y_tr[ts])\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "eval_res ={}\n",
    "gbdt = xgb.train(xgb_params, dtrain,\n",
    "                 num_rounds, watchlist,\n",
    "                 early_stopping_rounds=500,\n",
    "                 verbose_eval=50,\n",
    "                 evals_result=eval_res)\n",
    "print (datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.547439, 0.547439, 0.547442]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_res['eval']['rmse'][-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7017"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_res['eval']['rmse'].__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! блендинг наше все!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Генерация сабмишиона :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(dt_tr, label=y_tr)\n",
    "dtest = xgb.DMatrix(dt_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9h 1min 13s, sys: 10.6 s, total: 9h 1min 24s\n",
      "Wall time: 1h 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xgb_params = {\n",
    "  'objective': 'reg:linear',\n",
    "    'max_depth': 15,\n",
    "    'eta': 0.03,\n",
    "    'booster': 'gbtree',\n",
    "    'seed': 1,\n",
    "    'alpha': 0.5,\n",
    "    'lambda': 1.4,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'colsample_bylevel': 1.0,\n",
    "    'subsample': 0.9,\n",
    "    'min_child_weight': 9,\n",
    "    'silent': 1,\n",
    "    #'nthread':11,\n",
    "    'eval_metric':'rmse'\n",
    "}\n",
    "\n",
    "num_rounds = 9910\n",
    "\n",
    "gbdt = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "ans = gbdt.predict(dtest)\n",
    "real_ans1 = np.exp(ans)-1.\n",
    "gbdt.save_model('xgb.blend1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7h 49min 43s, sys: 9.14 s, total: 7h 49min 52s\n",
      "Wall time: 53min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xgb_params = {\n",
    "  'objective': 'reg:linear',\n",
    "    'max_depth': 13,\n",
    "    'eta': 0.028,\n",
    "    'booster': 'gbtree',\n",
    "    'seed': 1,\n",
    "    'alpha': 0.1,\n",
    "    'lambda': 1.1,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'colsample_bylevel': 1.0,\n",
    "    'subsample': 0.99,\n",
    "    'min_child_weight': 2,\n",
    "    'silent': 1,\n",
    "    #'nthread':11,\n",
    "    'eval_metric':'rmse'\n",
    "}\n",
    "\n",
    "num_rounds = 10000\n",
    "\n",
    "gbdt1 = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "ans1 = gbdt1.predict(dtest)\n",
    "real_ans2 = np.exp(ans1)-1.\n",
    "\n",
    "gbdt1.save_model('xgb.blend2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del gbdt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgb_params = {\n",
    "  'objective': 'reg:linear',\n",
    "    'max_depth': 19,\n",
    "    'eta': 0.28,\n",
    "    'booster': 'gbtree',\n",
    "    'seed': 1,\n",
    "    'alpha': 0.0,\n",
    "    'lambda': 1.3,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'subsample': 0.8,\n",
    "    'min_child_weight': 9,\n",
    "    'silent': 1,\n",
    "    #'nthread':11,\n",
    "    'eval_metric':'rmse'\n",
    "}\n",
    "\n",
    "num_rounds = 8000\n",
    "\n",
    "gbdt2 = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "\n",
    "ans = gbdt2.predict(dtest)\n",
    "real_ans3 = np.exp(ans)-1.\n",
    "\n",
    "gbdt2.save_model('xgb.blend3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del gbdt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 17,\n",
    "    'eta': 0.029,\n",
    "    'gamma' : 0,\n",
    "    'booster': 'gbtree',\n",
    "    'seed': 1,\n",
    "    'alpha': 0.8,\n",
    "    'lambda': 1.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bylevel': 1.0,\n",
    "    'subsample': 0.7,\n",
    "    'min_child_weight': 1,\n",
    "    'silent': 1,\n",
    "    'nthread': 10,\n",
    "    'eval_metric':'rmse'\n",
    "}\n",
    "\n",
    "num_rounds = 5585\n",
    "\n",
    "gbdt4 = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "\n",
    "ans = gbdt4.predict(dtest)\n",
    "real_ans4 = np.exp(ans)-1.\n",
    "\n",
    "gbdt4.save_model('xgb.blend4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del gbdt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 15,\n",
    "    'eta': 0.029,\n",
    "    'gamma' : 0,\n",
    "    'booster': 'gbtree',\n",
    "    'seed': 1,\n",
    "    'alpha': 2.0,\n",
    "    'lambda': 0.1,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'colsample_bylevel': 1.0,\n",
    "    'subsample': 0.95,\n",
    "    'min_child_weight': 1,\n",
    "    'silent': 1,\n",
    "    'nthread': 11,\n",
    "    'eval_metric':'rmse'\n",
    "}\n",
    "\n",
    "num_rounds = 7000\n",
    "\n",
    "\n",
    "gbdt5 = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "\n",
    "ans = gbdt5.predict(dtest)\n",
    "real_ans5 = np.exp(ans)-1.\n",
    "\n",
    "gbdt5.save_model('xgb.blend5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del gbdt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418991,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_ans2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_data = pd.read_csv('test.csv', sep=';' , usecols=['id'])\n",
    "tmp_data['item_views'] =  0.35 * real_ans1 + 0.10 * real_ans2 + \\\n",
    "                          0.10 * real_ans3 + 0.10 * real_ans4 + \\\n",
    "                          0.35 * real_ans5\n",
    "tmp_data.to_csv('ans_amir_10+.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  - третье место на лидерборде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "метрика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=1}^{n} \\sqrt{\\frac{1}{n} \\big{(} log(p_i+1) - log(y_i+1) \\big{)} } $$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def RMSLE(y_true, pred):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(pred) == xgb.core.DMatrix:\n",
    "        pred = pred.get_label()\n",
    "        res = np.sqrt(np.mean((np.log(pred+1.) - np.log(y_true+1.))**2))\n",
    "        return 'RMSLE', res\n",
    "    else:\n",
    "        res = np.sqrt(np.mean((np.log(pred+1.) - np.log(y_true+1.))**2))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
