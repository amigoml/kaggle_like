{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./x_train.csv', header=None )#, na_values='?')\n",
    "train_y=pd.read_csv('./y_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=train.columns.values\n",
    "\n",
    "for i in n:\n",
    "    for j in n:\n",
    "        if i!=j:\n",
    "            train[str(i)+'&'+str(j)]=train[i]*train[j]\n",
    "            k=train[i]+train[j]\n",
    "            k[k>0]=1\n",
    "            train[str(i)+'|'+str(j)]=k\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "train['target']=train_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x13188c490>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAFhCAYAAABecfxwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOhgjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13188c590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pairplot(train,hue='target',vars=[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt=train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### создадим себе выборки хорошие для тренировок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210,)\n",
      "(1770, 210)\n"
     ]
    }
   ],
   "source": [
    "y=np.hstack(np.array([dt['target'].values.astype(int)]))\n",
    "print y.shape\n",
    "\n",
    "n=list(dt.columns.values)\n",
    "n=n[:n.index('target')]+n[n.index('target')+1:]\n",
    "\n",
    "x= np.vstack([np.array(dt[n[0]].values)])\n",
    "for i in n[1:]:\n",
    "    x=np.append(x, np.vstack([np.array(dt[i].values)]),axis=0)\n",
    "print x.shape\n",
    "x=x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_scaled=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import grid_search\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_rf = {'n_estimators':[200,250,300,500], 'random_state':[1,2] , 'min_samples_leaf':[1,3]}\n",
    "rf = RandomForestClassifier(n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_gs=grid_search.GridSearchCV(rf,params_rf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid={'n_estimators': [200, 250, 300, 500], 'random_state': [1, 2], 'min_samples_leaf': [1, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.fit(x_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47619047619\n",
      "{'n_estimators': 200, 'random_state': 2, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "print rf_gs.best_score_\n",
    "print rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.456301218162\n"
     ]
    }
   ],
   "source": [
    "a=cross_val_score(RandomForestClassifier(n_jobs=2, n_estimators=200, random_state=2, bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, oob_score=False, verbose=0, warm_start=False), x_scaled, y, cv=5, scoring='accuracy')\n",
    "print np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=2,\n",
       "            oob_score=False, random_state=2, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=2,n_estimators=200,random_state=2,min_samples_leaf=1)\n",
    "rf.fit(x_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18&19 0.00352857663045\n",
      "28&27 0.00289384532473\n",
      "27&21 0.00239106148303\n",
      "20&21 0.00228879397187\n",
      "12|22 0.00228531817487\n",
      "14|12 0.0022413547626\n",
      "27&28 0.00219380254253\n",
      "26&6 0.00217216068119\n",
      "12|14 0.00211770182137\n",
      "0|20 0.00185875299907\n",
      "12|6 0.00182087252061\n",
      "16&19 0.00181888550858\n",
      "3&19 0.00178808862892\n",
      "14|4 0.00174492263776\n",
      "6|28 0.00172699263748\n",
      "20|13 0.0017216402626\n",
      "12&19 0.00168113834248\n",
      "14&1 0.00166912281704\n",
      "1|12 0.00164358892077\n",
      "17|12 0.00161874672292\n",
      "1|6 0.0016156668049\n",
      "0 0.00161223661861\n",
      "14|15 0.00160975965502\n",
      "16&28 0.0016058793449\n",
      "26&13 0.00160357358591\n",
      "21&19"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-f13cb15f2902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfeature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m         all_importances = Parallel(n_jobs=self.n_jobs, backend=\"threading\")(\n\u001b[1;32m    312\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature_importances_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             for tree in self.estimators_)\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_importances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((tree,))\u001b[0m\n\u001b[1;32m    311\u001b[0m         all_importances = Parallel(n_jobs=self.n_jobs, backend=\"threading\")(\n\u001b[1;32m    312\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature_importances_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             for tree in self.estimators_)\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_importances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdelayed\u001b[0;34m(function, check_pickle)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mdelayed_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\" functools.wraps fails on some callable objects \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/functools.pyc\u001b[0m in \u001b[0;36mupdate_wrapper\u001b[0;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Return the wrapper so this can be used as a decorator via partial()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "indexes= np.argsort(rf.feature_importances_)\n",
    "\n",
    "for i in indexes[::-1]:\n",
    "    print train.columns.values[i], rf.feature_importances_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_svm = dict(gamma=[ 0.0001, 0.001, 0.1, 1, 10], C=[1,4,5,6] )\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, param_svm,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid={'C': [1, 4, 5, 6], 'gamma': [0.0001, 0.001, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47619047619\n",
      "{'C': 5, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print grid.best_score_\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.441777408638\n"
     ]
    }
   ],
   "source": [
    "print np.mean(cross_val_score(svm.SVC(C = 5, gamma = 0.1, kernel='rbf'), x_scaled, y, cv=5,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_knn={'n_neighbors': [1,5,10,20,42,43,44,50]}\n",
    "knn=KNeighborsClassifier()\n",
    "gs3=grid_search.GridSearchCV(knn, params_knn, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_neighbors=5, p=2, weights='uniform'),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [5]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, score_func=None, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3.fit(x_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.404761904762\n",
      "{'n_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "print gs3.best_score_\n",
    "print gs3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.433648947951\n"
     ]
    }
   ],
   "source": [
    "print np.mean(cross_val_score(KNeighborsClassifier(n_neighbors=15), x_scaled, y, cv=5,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формируем тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv('./x_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=test.columns.values\n",
    "\n",
    "for i in n:\n",
    "    for j in n:\n",
    "        if i!=j:\n",
    "            test[str(i)+'&'+str(j)]=test[i]*test[j]\n",
    "            k=test[i]+test[j]\n",
    "            k[k>0]=1\n",
    "            test[str(i)+'|'+str(j)]=k\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1770, 210)\n"
     ]
    }
   ],
   "source": [
    "dt=test.copy()\n",
    "n=list(dt.columns.values)\n",
    "\n",
    "x= np.vstack([np.array(dt[n[0]].values)])\n",
    "for i in n[1:]:\n",
    "    x=np.append(x, np.vstack([np.array(dt[i].values)]),axis=0)\n",
    "print x.shape\n",
    "x=x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = pd.Series(rf.predict(x))\n",
    "res=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#res['id']=test[0]\n",
    "res['y']=ans\n",
    "res\n",
    "res.to_csv('ans.txt', index=False, header=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### (n_jobs=2,n_estimators=200,random_state=2,min_samples_leaf=1)\n",
    "0,429\n",
    "\n",
    "\n",
    "##### без фичер инжениринга свм 444 гридсерч дал."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attrib_names = [ str(i) for i in range(0,29) ]\n",
    "\n",
    "def make_intent(example):\n",
    "    global attrib_names\n",
    "    return set([i+':'+str(k) for i, k in zip(attrib_names, example)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    \n",
    "    data_0=[]\n",
    "    data_1=[]\n",
    "    data_2=[]\n",
    "    \n",
    "    x_test=[]\n",
    "    y_test=[]\n",
    "\n",
    "    train=pd.read_csv('./x_train.csv', header=None )#, na_values='?')\n",
    "    train_y=pd.read_csv('./y_train.csv', header=None)\n",
    "    train['target']=train_y[0]\n",
    "    \n",
    "    dt1=train[train.index < 210. * 4 / 5].copy()\n",
    "    dt2=train[train.index >= 210. * 4 / 5].copy()\n",
    "    \n",
    "    ###_________###\n",
    "    \n",
    "    train = np.array(dt1)\n",
    "    \n",
    "    test = np.array(dt2)\n",
    "    ###_________###\n",
    "    \n",
    "    \n",
    "#    train=np.array(train)\n",
    "\n",
    "    data_0 = [make_intent(a[:-1]) for a in train if a[-1] == 0]\n",
    "    data_1 = [make_intent(a[:-1]) for a in train if a[-1] == 1]\n",
    "    data_2 = [make_intent(a[:-1]) for a in train if a[-1] == 2]\n",
    "   \n",
    "#    test=pd.read_csv('./x_test.csv', header=None)    \n",
    "#    test=np.array(test)\n",
    "#    x_test = [make_intent(a) for a in test]\n",
    "    x_test = [make_intent(a[:-1]) for a in test]\n",
    "    y_test = [a[-1] for a in test]\n",
    "    \n",
    "    return data_0, data_1, data_2, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos, neg, eq, x_test, y_test = read_files()\n",
    "print is_in_intent(pos, neg, eq, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_in_intent(plus, minus, ravno, x_test, y_test):\n",
    "    y_pred=[]\n",
    "    counter=0\n",
    "    for i in x_test:\n",
    "        counter+=1\n",
    "        unkn_set=i\n",
    "        pos=0\n",
    "        neg=0\n",
    "        eq=0\n",
    "\n",
    "        for j in plus:\n",
    "            pos_set=j\n",
    "            res=pos_set & unkn_set\n",
    "            closure=0\n",
    "            for k in minus:\n",
    "                if k.issuperset(res):\n",
    "                    closure+=1\n",
    "                    break  \n",
    "            for k in ravno:\n",
    "                if k.issuperset(res):\n",
    "                    closure+=1\n",
    "                    break\n",
    "            if closure==0:\n",
    "                pos+=float(len(res)) / len(pos_set)\n",
    "        pos=float(pos) / len(plus)   \n",
    "        \n",
    "        \n",
    "        for j in minus:\n",
    "            neg_set=j\n",
    "            res=neg_set & unkn_set\n",
    "            closure=0\n",
    "            for k in plus:\n",
    "                if k.issuperset(res):\n",
    "                    closure+=1\n",
    "                    break\n",
    "            for k in ravno:\n",
    "                if k.issuperset(res):\n",
    "                    closure+=1\n",
    "                    break\n",
    "            if closure==0:\n",
    "                neg+=float(len(res)) / len(neg_set)\n",
    "        neg=float(neg) / len(minus) \n",
    "\n",
    "        \n",
    "        for j in ravno:\n",
    "            neg_set=j\n",
    "            res=neg_set & unkn_set\n",
    "            closure=0\n",
    "            for k in plus:\n",
    "                if k.issuperset(res):\n",
    "                    closure+=1\n",
    "                    break\n",
    "            for k in minus:\n",
    "                if k.issuperset(res):\n",
    "                    closure+=1\n",
    "                    break\n",
    "            if closure==0:\n",
    "                eq+=float(len(res)) / len(neg_set)\n",
    "        eq=float(eq) / len(minus) \n",
    "        \n",
    "\n",
    "        if (pos <= neg and pos <= eq):\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            if (neg <= pos and neg <= eq):\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                if (eq <= pos and eq <= neg):\n",
    "                    y_pred.append(2)\n",
    "            \n",
    "            \n",
    "\n",
    "    y_pred=np.array(y_pred)\n",
    "    y_test=np.array(y_test)\n",
    "    \n",
    "    print y_pred\n",
    "    print y_test\n",
    "    \n",
    "    return print_metris(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def print_metris(y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import cross_validation\n",
    "from time import time\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBModel, XGBClassifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l=[str(i) for i in range(0,30)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./x_train.csv', header=None, names=l) #, na_values='?')\n",
    "train_y=pd.read_csv('./y_train.csv', header=None)\n",
    "\n",
    "n=train.columns.values\n",
    "\n",
    "for i in n:\n",
    "    for j in n:\n",
    "        if i!=j:\n",
    "            train[str(i)+'111'+str(j)]=train[i]*train[j]\n",
    "            k=train[i]+train[j]\n",
    "            k[k>0]=1\n",
    "            train[str(i)+'000'+str(j)]=k\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features=train.columns.values\n",
    "data_train=train.copy()\n",
    "data_train['y']=train_y[0]\n",
    "\n",
    "train['y']=train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Определяем кросс-валидацию для стакинга и последующей кросс-валидации итоговой модели. \n",
    "cv = cross_validation.StratifiedKFold(data_train['y'], n_folds=3, shuffle=True,\n",
    "                               random_state=0)\n",
    "\n",
    "\n",
    "#Определяем классификаторы для стакинга. Здесь я поставил намеренно слабые, чтобы они считались быстрее.\n",
    "#Имеет смысл поиграть с параметрами и посмотреть, что получится. \n",
    "\n",
    "svmP=svm.SVC(C = 5, gamma = 0.1, kernel='rbf',probability=True)\n",
    "\n",
    "clf_rf = ensemble.RandomForestClassifier(n_estimators = 300, n_jobs=4, \n",
    "                                         max_depth = 10, max_features=0.35,\n",
    "                                         random_state=1, min_samples_leaf=2)\n",
    "\n",
    "clf_weak_rf = ensemble.RandomForestClassifier(n_estimators = 200, n_jobs=4, \n",
    "                                         max_depth = 10, max_features=0.35,\n",
    "                                         random_state=1, min_samples_leaf=2)\n",
    "\n",
    "#В этом примере я использовал адабуст на random forest\n",
    "clf_ada = ensemble.AdaBoostClassifier(base_estimator = svmP, n_estimators=50, learning_rate=0.01, \n",
    "                                      random_state=0)\n",
    "\n",
    "clf_ada1 = ensemble.AdaBoostClassifier(base_estimator = clf_rf, n_estimators=100, learning_rate=0.001, \n",
    "                                      random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Определяем оригинальный хгбуст\n",
    "params = {\"objective\": 'multi:softprob',\n",
    "          \"eta\": 0.04,\n",
    "          \"max_depth\": 5,\n",
    "          \"min_child_weight\": 7,\n",
    "          \"silent\": 1,\n",
    "          \"subsample\": 0.4,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"seed\": 1,\n",
    "          'lambda':7.0,\n",
    "          'alpha':0,\n",
    "          'num_class': nnn,\n",
    "          'gamma': 0,\n",
    "          'nthread':4\n",
    "         }\n",
    "num_trees=10\n",
    "\n",
    "nnn=len(train['y'].unique())\n",
    "\n",
    "\n",
    "svm_preds = pd.DataFrame(index=train.index, columns=['SVM_'+str(i) for i in range(nnn)])\n",
    "rf_preds = pd.DataFrame(index=train.index, columns=['rf_'+str(i) for i in range(nnn)])\n",
    "ada_preds = pd.DataFrame(index=train.index, columns=['ada_'+str(i) for i in range(nnn)])\n",
    "xgbst_preds = pd.DataFrame(index=train.index, columns=['xgbst_'+str(i) for i in range(nnn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Training svm\n",
      "   Done.\n",
      " Training RF\n",
      "   Done.\n",
      " Training AdaBoost\n",
      "   Done.\n",
      " Training AdaBoost\n",
      "   Done.\n",
      "Fold 2\n",
      " Training svm\n",
      "   Done.\n",
      " Training RF\n",
      "   Done.\n",
      " Training AdaBoost\n",
      "   Done.\n",
      " Training AdaBoost\n",
      "   Done.\n",
      "Fold 3\n",
      " Training svm\n",
      "   Done.\n",
      " Training RF\n",
      "   Done.\n",
      " Training AdaBoost\n",
      "   Done.\n",
      " Training AdaBoost\n",
      "   Done.\n"
     ]
    }
   ],
   "source": [
    "#Вероятности заполняются на кросс-валидации. Обратите внимание, что для оценки точности логистической регрессии\n",
    "#используется та же самая кросс-валидация\n",
    "i = 0\n",
    "for train_ind, test_ind in cv:\n",
    "    i+=1\n",
    "    print 'Fold', i\n",
    "    train_cv = train.loc[train_ind]\n",
    "    test_cv = train.loc[test_ind]\n",
    "    \n",
    "    print ' Training svm'\n",
    "    svmP.fit(train_cv[features], train_cv['y']) \n",
    "    svm_preds.loc[test_ind] = svmP.predict_proba(test_cv[features])\n",
    "    print '   Done.'\n",
    "    \n",
    "    \n",
    "    print ' Training RF'\n",
    "    clf_rf.fit(train_cv[features], train_cv['y']) \n",
    "    rf_preds.loc[test_ind] = clf_rf.predict_proba(test_cv[features])\n",
    "    print '   Done.'\n",
    "    \n",
    "    print ' Training AdaBoost'\n",
    "    clf_ada.fit(train_cv[features], train_cv['y'])\n",
    "    ada_preds.loc[test_ind] = clf_ada.predict_proba(test_cv[features])\n",
    "    print '   Done.'\n",
    "\n",
    "    print ' Training AdaBoost'\n",
    "    clf_ada1.fit(train_cv[features], train_cv['y'])\n",
    "    xgbst_preds.loc[test_ind] = clf_ada1.predict_proba(test_cv[features])\n",
    "    print '   Done.'\n",
    "    \n",
    "    \n",
    "#    print ' Training XGBoost'\n",
    "#    gbm = xgb.train(params, xgb.DMatrix(train_cv[features], train_cv['y']), num_trees)\n",
    "#    xgbst_preds.loc[test_ind] = gbm.predict(xgb.DMatrix(test_cv[features])).reshape(test_cv.shape[0], nnn)\n",
    "#    print '   Done.'\n",
    "    \n",
    "    \n",
    "#Объединяем вероятности в один датафрейм\n",
    "train1 = pd.concat([ada_preds, rf_preds, xgbst_preds, svm_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log regr accuracy 0.433592861517 ± 0.0430508975989\n"
     ]
    }
   ],
   "source": [
    "#Прогтейоняем логистическую регрессию на той же кросс-валидации, которая использовалась для подсчета вероятнос\n",
    "clf_lr = LogisticRegression(C=30, class_weight=None, \n",
    "                            penalty='l2', random_state=0,\n",
    "                            solver='lbfgs', multi_class = 'ovr')\n",
    "\n",
    "a = cross_val_score(clf_lr, train1, y=data_train['y'], cv=cv)\n",
    "print 'Log regr accuracy', np.mean(a), '±', np.std(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada_0</th>\n",
       "      <th>ada_1</th>\n",
       "      <th>ada_2</th>\n",
       "      <th>rf_0</th>\n",
       "      <th>rf_1</th>\n",
       "      <th>rf_2</th>\n",
       "      <th>xgbst_0</th>\n",
       "      <th>xgbst_1</th>\n",
       "      <th>xgbst_2</th>\n",
       "      <th>SVM_0</th>\n",
       "      <th>SVM_1</th>\n",
       "      <th>SVM_2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2046667</td>\n",
       "      <td>0.5547778</td>\n",
       "      <td>0.2405556</td>\n",
       "      <td>0.1900556</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.2839444</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.2325238</td>\n",
       "      <td>0.2058333</td>\n",
       "      <td>0.5616429</td>\n",
       "      <td>0.2106111</td>\n",
       "      <td>0.2268333</td>\n",
       "      <td>0.5625556</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.3122222</td>\n",
       "      <td>0.3227778</td>\n",
       "      <td>0.3448333</td>\n",
       "      <td>0.3502222</td>\n",
       "      <td>0.3049444</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>0.3785</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.2514444</td>\n",
       "      <td>0.3970556</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.1528333</td>\n",
       "      <td>0.5883333</td>\n",
       "      <td>0.2588333</td>\n",
       "      <td>0.1733333</td>\n",
       "      <td>0.5093889</td>\n",
       "      <td>0.3172778</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.3537698</td>\n",
       "      <td>0.3068968</td>\n",
       "      <td>0.3393333</td>\n",
       "      <td>0.2840556</td>\n",
       "      <td>0.3241667</td>\n",
       "      <td>0.3917778</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2732222</td>\n",
       "      <td>0.4892778</td>\n",
       "      <td>0.2375</td>\n",
       "      <td>0.2476111</td>\n",
       "      <td>0.4972778</td>\n",
       "      <td>0.2551111</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2108333</td>\n",
       "      <td>0.4129444</td>\n",
       "      <td>0.3762222</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.4534444</td>\n",
       "      <td>0.3415556</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.3010238</td>\n",
       "      <td>0.2988095</td>\n",
       "      <td>0.4001667</td>\n",
       "      <td>0.2987778</td>\n",
       "      <td>0.3131111</td>\n",
       "      <td>0.3881111</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.4539444</td>\n",
       "      <td>0.2441111</td>\n",
       "      <td>0.3019444</td>\n",
       "      <td>0.4769444</td>\n",
       "      <td>0.2442222</td>\n",
       "      <td>0.2788333</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.1114524</td>\n",
       "      <td>0.3650635</td>\n",
       "      <td>0.5234841</td>\n",
       "      <td>0.1431111</td>\n",
       "      <td>0.4173889</td>\n",
       "      <td>0.4395</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2286984</td>\n",
       "      <td>0.1433333</td>\n",
       "      <td>0.6279683</td>\n",
       "      <td>0.2043333</td>\n",
       "      <td>0.1176111</td>\n",
       "      <td>0.6780556</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2682778</td>\n",
       "      <td>0.3147222</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.2804841</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.3915159</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.1929762</td>\n",
       "      <td>0.3661111</td>\n",
       "      <td>0.4409127</td>\n",
       "      <td>0.1850397</td>\n",
       "      <td>0.3945714</td>\n",
       "      <td>0.4203889</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.2979444</td>\n",
       "      <td>0.4735556</td>\n",
       "      <td>0.2090635</td>\n",
       "      <td>0.2407381</td>\n",
       "      <td>0.5501984</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.3406667</td>\n",
       "      <td>0.4311111</td>\n",
       "      <td>0.2282222</td>\n",
       "      <td>0.3670079</td>\n",
       "      <td>0.4363333</td>\n",
       "      <td>0.1966587</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.2816667</td>\n",
       "      <td>0.4008333</td>\n",
       "      <td>0.2958333</td>\n",
       "      <td>0.2528333</td>\n",
       "      <td>0.4513333</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2090556</td>\n",
       "      <td>0.3241111</td>\n",
       "      <td>0.4668333</td>\n",
       "      <td>0.188373</td>\n",
       "      <td>0.3164444</td>\n",
       "      <td>0.4951825</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.1671111</td>\n",
       "      <td>0.1777222</td>\n",
       "      <td>0.6551667</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.2446111</td>\n",
       "      <td>0.6013889</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.4338889</td>\n",
       "      <td>0.2315714</td>\n",
       "      <td>0.3345397</td>\n",
       "      <td>0.4335556</td>\n",
       "      <td>0.2260185</td>\n",
       "      <td>0.3404259</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.1846667</td>\n",
       "      <td>0.4702778</td>\n",
       "      <td>0.3450556</td>\n",
       "      <td>0.1772778</td>\n",
       "      <td>0.4796667</td>\n",
       "      <td>0.3430556</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2512222</td>\n",
       "      <td>0.3218889</td>\n",
       "      <td>0.4268889</td>\n",
       "      <td>0.2348889</td>\n",
       "      <td>0.3493889</td>\n",
       "      <td>0.4157222</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.4271111</td>\n",
       "      <td>0.3092778</td>\n",
       "      <td>0.2636111</td>\n",
       "      <td>0.4650556</td>\n",
       "      <td>0.3107778</td>\n",
       "      <td>0.2241667</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.3436111</td>\n",
       "      <td>0.3861111</td>\n",
       "      <td>0.2702778</td>\n",
       "      <td>0.3387222</td>\n",
       "      <td>0.3543333</td>\n",
       "      <td>0.3069444</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.4082222</td>\n",
       "      <td>0.3053333</td>\n",
       "      <td>0.2864444</td>\n",
       "      <td>0.4006111</td>\n",
       "      <td>0.3017963</td>\n",
       "      <td>0.2975926</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.30375</td>\n",
       "      <td>0.2985</td>\n",
       "      <td>0.39775</td>\n",
       "      <td>0.2925476</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.4224524</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.2968333</td>\n",
       "      <td>0.2767222</td>\n",
       "      <td>0.4264444</td>\n",
       "      <td>0.2956111</td>\n",
       "      <td>0.3119444</td>\n",
       "      <td>0.3924444</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2888889</td>\n",
       "      <td>0.1863889</td>\n",
       "      <td>0.5247222</td>\n",
       "      <td>0.3333889</td>\n",
       "      <td>0.1786111</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2186667</td>\n",
       "      <td>0.3427778</td>\n",
       "      <td>0.4385556</td>\n",
       "      <td>0.1860556</td>\n",
       "      <td>0.3488889</td>\n",
       "      <td>0.4650556</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.1990556</td>\n",
       "      <td>0.3815</td>\n",
       "      <td>0.4194444</td>\n",
       "      <td>0.1665</td>\n",
       "      <td>0.3918333</td>\n",
       "      <td>0.4416667</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2200556</td>\n",
       "      <td>0.3893889</td>\n",
       "      <td>0.3905556</td>\n",
       "      <td>0.2169444</td>\n",
       "      <td>0.3727778</td>\n",
       "      <td>0.4102778</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.2226349</td>\n",
       "      <td>0.5896429</td>\n",
       "      <td>0.1877222</td>\n",
       "      <td>0.1933333</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.2376667</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.3388333</td>\n",
       "      <td>0.2040556</td>\n",
       "      <td>0.4571111</td>\n",
       "      <td>0.3413333</td>\n",
       "      <td>0.1998333</td>\n",
       "      <td>0.4588333</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.3580556</td>\n",
       "      <td>0.4582778</td>\n",
       "      <td>0.1836667</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.4230317</td>\n",
       "      <td>0.2299683</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.5268889</td>\n",
       "      <td>0.2276667</td>\n",
       "      <td>0.2454444</td>\n",
       "      <td>0.4517593</td>\n",
       "      <td>0.2403519</td>\n",
       "      <td>0.3078889</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2784444</td>\n",
       "      <td>0.1811111</td>\n",
       "      <td>0.5404444</td>\n",
       "      <td>0.3196111</td>\n",
       "      <td>0.1919444</td>\n",
       "      <td>0.4884444</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2935</td>\n",
       "      <td>0.4378889</td>\n",
       "      <td>0.2686111</td>\n",
       "      <td>0.3669683</td>\n",
       "      <td>0.3525635</td>\n",
       "      <td>0.2804683</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.3550556</td>\n",
       "      <td>0.3202778</td>\n",
       "      <td>0.3246667</td>\n",
       "      <td>0.2835</td>\n",
       "      <td>0.3305317</td>\n",
       "      <td>0.3859683</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.5536667</td>\n",
       "      <td>0.2529444</td>\n",
       "      <td>0.1933889</td>\n",
       "      <td>0.5647222</td>\n",
       "      <td>0.2074444</td>\n",
       "      <td>0.2278333</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.1834444</td>\n",
       "      <td>0.4743889</td>\n",
       "      <td>0.3421667</td>\n",
       "      <td>0.1505926</td>\n",
       "      <td>0.5076111</td>\n",
       "      <td>0.3417963</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.3769921</td>\n",
       "      <td>0.4933333</td>\n",
       "      <td>0.1296746</td>\n",
       "      <td>0.3802778</td>\n",
       "      <td>0.4578333</td>\n",
       "      <td>0.1618889</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.2908016</td>\n",
       "      <td>0.2939206</td>\n",
       "      <td>0.4152778</td>\n",
       "      <td>0.3389206</td>\n",
       "      <td>0.2970344</td>\n",
       "      <td>0.364045</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2040556</td>\n",
       "      <td>0.2065556</td>\n",
       "      <td>0.5893889</td>\n",
       "      <td>0.2146667</td>\n",
       "      <td>0.1947778</td>\n",
       "      <td>0.5905556</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.1814444</td>\n",
       "      <td>0.4697778</td>\n",
       "      <td>0.3487778</td>\n",
       "      <td>0.1873333</td>\n",
       "      <td>0.4547778</td>\n",
       "      <td>0.3578889</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.2986111</td>\n",
       "      <td>0.3833889</td>\n",
       "      <td>0.3571111</td>\n",
       "      <td>0.3000556</td>\n",
       "      <td>0.3428333</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.4542222</td>\n",
       "      <td>0.3806667</td>\n",
       "      <td>0.1651111</td>\n",
       "      <td>0.4519259</td>\n",
       "      <td>0.3531111</td>\n",
       "      <td>0.194963</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.2805</td>\n",
       "      <td>0.2549921</td>\n",
       "      <td>0.4645079</td>\n",
       "      <td>0.2456111</td>\n",
       "      <td>0.2769444</td>\n",
       "      <td>0.4774444</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2831111</td>\n",
       "      <td>0.1701111</td>\n",
       "      <td>0.5467778</td>\n",
       "      <td>0.3634921</td>\n",
       "      <td>0.1497778</td>\n",
       "      <td>0.4867302</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2316111</td>\n",
       "      <td>0.4408333</td>\n",
       "      <td>0.3275556</td>\n",
       "      <td>0.2321111</td>\n",
       "      <td>0.4172222</td>\n",
       "      <td>0.3506667</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.5351111</td>\n",
       "      <td>0.1319444</td>\n",
       "      <td>0.3329444</td>\n",
       "      <td>0.4528333</td>\n",
       "      <td>0.1750556</td>\n",
       "      <td>0.3721111</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.2821667</td>\n",
       "      <td>0.3965556</td>\n",
       "      <td>0.3212778</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.3732222</td>\n",
       "      <td>0.3317778</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.4581667</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.3278333</td>\n",
       "      <td>0.4406579</td>\n",
       "      <td>0.2404444</td>\n",
       "      <td>0.3188977</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2592222</td>\n",
       "      <td>0.3266667</td>\n",
       "      <td>0.4141111</td>\n",
       "      <td>0.2720556</td>\n",
       "      <td>0.3070714</td>\n",
       "      <td>0.420873</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2238889</td>\n",
       "      <td>0.3811111</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.2580556</td>\n",
       "      <td>0.4085</td>\n",
       "      <td>0.3334444</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.1792778</td>\n",
       "      <td>0.5426667</td>\n",
       "      <td>0.2780556</td>\n",
       "      <td>0.1768333</td>\n",
       "      <td>0.5117407</td>\n",
       "      <td>0.3114259</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.2408889</td>\n",
       "      <td>0.5829444</td>\n",
       "      <td>0.1761667</td>\n",
       "      <td>0.2276667</td>\n",
       "      <td>0.4966111</td>\n",
       "      <td>0.2757222</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.34675</td>\n",
       "      <td>0.3298611</td>\n",
       "      <td>0.3233889</td>\n",
       "      <td>0.4130556</td>\n",
       "      <td>0.2785556</td>\n",
       "      <td>0.3083889</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.3101952</td>\n",
       "      <td>0.3455339</td>\n",
       "      <td>0.3442709</td>\n",
       "      <td>0.2678889</td>\n",
       "      <td>0.4862778</td>\n",
       "      <td>0.2458333</td>\n",
       "      <td>0.277619</td>\n",
       "      <td>0.4964365</td>\n",
       "      <td>0.2259444</td>\n",
       "      <td>0.3147378</td>\n",
       "      <td>0.3295275</td>\n",
       "      <td>0.3557347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.3467524</td>\n",
       "      <td>0.3207227</td>\n",
       "      <td>0.3325249</td>\n",
       "      <td>0.2937619</td>\n",
       "      <td>0.158373</td>\n",
       "      <td>0.5478651</td>\n",
       "      <td>0.4003148</td>\n",
       "      <td>0.1531667</td>\n",
       "      <td>0.4465185</td>\n",
       "      <td>0.309642</td>\n",
       "      <td>0.3339901</td>\n",
       "      <td>0.3563679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.3098979</td>\n",
       "      <td>0.3501795</td>\n",
       "      <td>0.3399226</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.1745</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.5466905</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.2613095</td>\n",
       "      <td>0.3204131</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.3504679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ada_0      ada_1      ada_2       rf_0       rf_1       rf_2  \\\n",
       "0    0.3101952  0.3455339  0.3442709  0.2046667  0.5547778  0.2405556   \n",
       "1    0.3467524  0.3207227  0.3325249  0.2325238  0.2058333  0.5616429   \n",
       "2    0.3098979  0.3501795  0.3399226      0.365  0.3122222  0.3227778   \n",
       "3    0.3098979  0.3501795  0.3399226     0.2725     0.3785      0.349   \n",
       "4    0.3101952  0.3455339  0.3442709  0.1528333  0.5883333  0.2588333   \n",
       "5    0.3467524  0.3207227  0.3325249  0.3537698  0.3068968  0.3393333   \n",
       "6    0.3101952  0.3455339  0.3442709  0.2732222  0.4892778     0.2375   \n",
       "7    0.3101952  0.3455339  0.3442709  0.2108333  0.4129444  0.3762222   \n",
       "8    0.3467524  0.3207227  0.3325249  0.3010238  0.2988095  0.4001667   \n",
       "9    0.3101952  0.3455339  0.3442709  0.4539444  0.2441111  0.3019444   \n",
       "10   0.3098979  0.3501795  0.3399226  0.1114524  0.3650635  0.5234841   \n",
       "11   0.3101952  0.3455339  0.3442709  0.2286984  0.1433333  0.6279683   \n",
       "12   0.3098979  0.3501795  0.3399226  0.2682778  0.3147222      0.417   \n",
       "13   0.3101952  0.3455339  0.3442709  0.1929762  0.3661111  0.4409127   \n",
       "14   0.3101952  0.3455339  0.3442709     0.2285  0.2979444  0.4735556   \n",
       "15   0.3467524  0.3207227  0.3325249  0.3406667  0.4311111  0.2282222   \n",
       "16   0.3467524  0.3207227  0.3325249     0.3175  0.2816667  0.4008333   \n",
       "17   0.3098979  0.3501795  0.3399226  0.2090556  0.3241111  0.4668333   \n",
       "18   0.3098979  0.3501795  0.3399226  0.1671111  0.1777222  0.6551667   \n",
       "19   0.3467524  0.3207227  0.3325249  0.4338889  0.2315714  0.3345397   \n",
       "20   0.3467524  0.3207227  0.3325249  0.1846667  0.4702778  0.3450556   \n",
       "21   0.3101952  0.3455339  0.3442709  0.2512222  0.3218889  0.4268889   \n",
       "22   0.3467524  0.3207227  0.3325249  0.4271111  0.3092778  0.2636111   \n",
       "23   0.3098979  0.3501795  0.3399226  0.3436111  0.3861111  0.2702778   \n",
       "24   0.3467524  0.3207227  0.3325249  0.4082222  0.3053333  0.2864444   \n",
       "25   0.3098979  0.3501795  0.3399226    0.30375     0.2985    0.39775   \n",
       "26   0.3467524  0.3207227  0.3325249  0.2968333  0.2767222  0.4264444   \n",
       "27   0.3101952  0.3455339  0.3442709  0.2888889  0.1863889  0.5247222   \n",
       "28   0.3098979  0.3501795  0.3399226  0.2186667  0.3427778  0.4385556   \n",
       "29   0.3467524  0.3207227  0.3325249  0.1990556     0.3815  0.4194444   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "180  0.3101952  0.3455339  0.3442709  0.2200556  0.3893889  0.3905556   \n",
       "181  0.3467524  0.3207227  0.3325249  0.2226349  0.5896429  0.1877222   \n",
       "182  0.3098979  0.3501795  0.3399226  0.3388333  0.2040556  0.4571111   \n",
       "183  0.3101952  0.3455339  0.3442709  0.3580556  0.4582778  0.1836667   \n",
       "184  0.3467524  0.3207227  0.3325249  0.5268889  0.2276667  0.2454444   \n",
       "185  0.3098979  0.3501795  0.3399226  0.2784444  0.1811111  0.5404444   \n",
       "186  0.3098979  0.3501795  0.3399226     0.2935  0.4378889  0.2686111   \n",
       "187  0.3101952  0.3455339  0.3442709  0.3550556  0.3202778  0.3246667   \n",
       "188  0.3101952  0.3455339  0.3442709  0.5536667  0.2529444  0.1933889   \n",
       "189  0.3098979  0.3501795  0.3399226  0.1834444  0.4743889  0.3421667   \n",
       "190  0.3098979  0.3501795  0.3399226  0.3769921  0.4933333  0.1296746   \n",
       "191  0.3467524  0.3207227  0.3325249  0.2908016  0.2939206  0.4152778   \n",
       "192  0.3098979  0.3501795  0.3399226  0.2040556  0.2065556  0.5893889   \n",
       "193  0.3098979  0.3501795  0.3399226  0.1814444  0.4697778  0.3487778   \n",
       "194  0.3098979  0.3501795  0.3399226      0.318  0.2986111  0.3833889   \n",
       "195  0.3467524  0.3207227  0.3325249  0.4542222  0.3806667  0.1651111   \n",
       "196  0.3467524  0.3207227  0.3325249     0.2805  0.2549921  0.4645079   \n",
       "197  0.3098979  0.3501795  0.3399226  0.2831111  0.1701111  0.5467778   \n",
       "198  0.3098979  0.3501795  0.3399226  0.2316111  0.4408333  0.3275556   \n",
       "199  0.3101952  0.3455339  0.3442709  0.5351111  0.1319444  0.3329444   \n",
       "200  0.3098979  0.3501795  0.3399226  0.2821667  0.3965556  0.3212778   \n",
       "201  0.3101952  0.3455339  0.3442709  0.4581667      0.214  0.3278333   \n",
       "202  0.3101952  0.3455339  0.3442709  0.2592222  0.3266667  0.4141111   \n",
       "203  0.3101952  0.3455339  0.3442709  0.2238889  0.3811111      0.395   \n",
       "204  0.3467524  0.3207227  0.3325249  0.1792778  0.5426667  0.2780556   \n",
       "205  0.3467524  0.3207227  0.3325249  0.2408889  0.5829444  0.1761667   \n",
       "206  0.3098979  0.3501795  0.3399226    0.34675  0.3298611  0.3233889   \n",
       "207  0.3101952  0.3455339  0.3442709  0.2678889  0.4862778  0.2458333   \n",
       "208  0.3467524  0.3207227  0.3325249  0.2937619   0.158373  0.5478651   \n",
       "209  0.3098979  0.3501795  0.3399226      0.548     0.1745     0.2775   \n",
       "\n",
       "       xgbst_0    xgbst_1    xgbst_2      SVM_0      SVM_1      SVM_2  y  \n",
       "0    0.1900556      0.526  0.2839444  0.3147378  0.3295275  0.3557347  1  \n",
       "1    0.2106111  0.2268333  0.5625556   0.309642  0.3339901  0.3563679  1  \n",
       "2    0.3448333  0.3502222  0.3049444  0.3204131   0.329119  0.3504679  1  \n",
       "3    0.2514444  0.3970556     0.3515  0.3204131   0.329119  0.3504679  0  \n",
       "4    0.1733333  0.5093889  0.3172778  0.3147378  0.3295275  0.3557347  2  \n",
       "5    0.2840556  0.3241667  0.3917778   0.309642  0.3339901  0.3563679  2  \n",
       "6    0.2476111  0.4972778  0.2551111  0.3147378  0.3295275  0.3557347  0  \n",
       "7        0.205  0.4534444  0.3415556  0.3147378  0.3295275  0.3557347  2  \n",
       "8    0.2987778  0.3131111  0.3881111   0.309642  0.3339901  0.3563679  0  \n",
       "9    0.4769444  0.2442222  0.2788333  0.3147378  0.3295275  0.3557347  1  \n",
       "10   0.1431111  0.4173889     0.4395  0.3204131   0.329119  0.3504679  2  \n",
       "11   0.2043333  0.1176111  0.6780556  0.3147378  0.3295275  0.3557347  1  \n",
       "12   0.2804841      0.328  0.3915159  0.3204131   0.329119  0.3504679  1  \n",
       "13   0.1850397  0.3945714  0.4203889  0.3147378  0.3295275  0.3557347  1  \n",
       "14   0.2090635  0.2407381  0.5501984  0.3147378  0.3295275  0.3557347  0  \n",
       "15   0.3670079  0.4363333  0.1966587   0.309642  0.3339901  0.3563679  1  \n",
       "16   0.2958333  0.2528333  0.4513333   0.309642  0.3339901  0.3563679  0  \n",
       "17    0.188373  0.3164444  0.4951825  0.3204131   0.329119  0.3504679  2  \n",
       "18       0.154  0.2446111  0.6013889  0.3204131   0.329119  0.3504679  0  \n",
       "19   0.4335556  0.2260185  0.3404259   0.309642  0.3339901  0.3563679  2  \n",
       "20   0.1772778  0.4796667  0.3430556   0.309642  0.3339901  0.3563679  2  \n",
       "21   0.2348889  0.3493889  0.4157222  0.3147378  0.3295275  0.3557347  0  \n",
       "22   0.4650556  0.3107778  0.2241667   0.309642  0.3339901  0.3563679  1  \n",
       "23   0.3387222  0.3543333  0.3069444  0.3204131   0.329119  0.3504679  0  \n",
       "24   0.4006111  0.3017963  0.2975926   0.309642  0.3339901  0.3563679  2  \n",
       "25   0.2925476      0.285  0.4224524  0.3204131   0.329119  0.3504679  0  \n",
       "26   0.2956111  0.3119444  0.3924444   0.309642  0.3339901  0.3563679  1  \n",
       "27   0.3333889  0.1786111      0.488  0.3147378  0.3295275  0.3557347  0  \n",
       "28   0.1860556  0.3488889  0.4650556  0.3204131   0.329119  0.3504679  2  \n",
       "29      0.1665  0.3918333  0.4416667   0.309642  0.3339901  0.3563679  0  \n",
       "..         ...        ...        ...        ...        ...        ... ..  \n",
       "180  0.2169444  0.3727778  0.4102778  0.3147378  0.3295275  0.3557347  1  \n",
       "181  0.1933333      0.569  0.2376667   0.309642  0.3339901  0.3563679  0  \n",
       "182  0.3413333  0.1998333  0.4588333  0.3204131   0.329119  0.3504679  0  \n",
       "183      0.347  0.4230317  0.2299683  0.3147378  0.3295275  0.3557347  0  \n",
       "184  0.4517593  0.2403519  0.3078889   0.309642  0.3339901  0.3563679  2  \n",
       "185  0.3196111  0.1919444  0.4884444  0.3204131   0.329119  0.3504679  2  \n",
       "186  0.3669683  0.3525635  0.2804683  0.3204131   0.329119  0.3504679  2  \n",
       "187     0.2835  0.3305317  0.3859683  0.3147378  0.3295275  0.3557347  1  \n",
       "188  0.5647222  0.2074444  0.2278333  0.3147378  0.3295275  0.3557347  0  \n",
       "189  0.1505926  0.5076111  0.3417963  0.3204131   0.329119  0.3504679  1  \n",
       "190  0.3802778  0.4578333  0.1618889  0.3204131   0.329119  0.3504679  2  \n",
       "191  0.3389206  0.2970344   0.364045   0.309642  0.3339901  0.3563679  0  \n",
       "192  0.2146667  0.1947778  0.5905556  0.3204131   0.329119  0.3504679  0  \n",
       "193  0.1873333  0.4547778  0.3578889  0.3204131   0.329119  0.3504679  1  \n",
       "194  0.3571111  0.3000556  0.3428333  0.3204131   0.329119  0.3504679  1  \n",
       "195  0.4519259  0.3531111   0.194963   0.309642  0.3339901  0.3563679  1  \n",
       "196  0.2456111  0.2769444  0.4774444   0.309642  0.3339901  0.3563679  2  \n",
       "197  0.3634921  0.1497778  0.4867302  0.3204131   0.329119  0.3504679  2  \n",
       "198  0.2321111  0.4172222  0.3506667  0.3204131   0.329119  0.3504679  1  \n",
       "199  0.4528333  0.1750556  0.3721111  0.3147378  0.3295275  0.3557347  0  \n",
       "200      0.295  0.3732222  0.3317778  0.3204131   0.329119  0.3504679  1  \n",
       "201  0.4406579  0.2404444  0.3188977  0.3147378  0.3295275  0.3557347  1  \n",
       "202  0.2720556  0.3070714   0.420873  0.3147378  0.3295275  0.3557347  0  \n",
       "203  0.2580556     0.4085  0.3334444  0.3147378  0.3295275  0.3557347  1  \n",
       "204  0.1768333  0.5117407  0.3114259   0.309642  0.3339901  0.3563679  1  \n",
       "205  0.2276667  0.4966111  0.2757222   0.309642  0.3339901  0.3563679  0  \n",
       "206  0.4130556  0.2785556  0.3083889  0.3204131   0.329119  0.3504679  1  \n",
       "207   0.277619  0.4964365  0.2259444  0.3147378  0.3295275  0.3557347  1  \n",
       "208  0.4003148  0.1531667  0.4465185   0.309642  0.3339901  0.3563679  2  \n",
       "209  0.5466905      0.192  0.2613095  0.3204131   0.329119  0.3504679  0  \n",
       "\n",
       "[210 rows x 13 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr=train1.copy()\n",
    "tr['y']=train['y']\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
