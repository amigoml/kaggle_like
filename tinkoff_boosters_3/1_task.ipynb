{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В credit_train.csv содержится 170746 строк с данными о клиентах сети магазинов электроники, в этих магазинах они подали заявки на кредит. Колонка open_account_flg содержит 1 если клиент выбрал Тинькофф и 0 в противном случае. В credit_test.csv содержится 91940 строк с данными, для каждой строки следует предсказать возьмет ли соответствующий ей человек кредит в Тинькофф."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "education\tОбразование\n",
    "SCH\tНачальное, среднее  \n",
    "PGR\tВторое высшее     \n",
    "GRD\tВысшее           \n",
    "UGR\tНеполное высшее   \n",
    "ACD\tУченая степень     \n",
    "\n",
    "\n",
    "job_position\tРабота\n",
    "SPC\tНеруководящий сотрудник - специалист\n",
    "DIR\tРуководитель организации\n",
    "HSK\tДомохозяйка\n",
    "INV\tНе работает (инвалидность)\n",
    "WOI\tРаботает на ИП\n",
    "WRK\tНеруководящий сотрудник - рабочий\n",
    "ATP\tНеруководящий сотрудник - обслуживающий персонал\n",
    "WRP\tРаботающий пенсионер\n",
    "UMN\tРуководитель подразделения\n",
    "NOR\tНе работает\n",
    "PNS\tПенсионер\n",
    "BIS\tСобственный бизнес\n",
    "INP\tИндивидуальный предприниматель\n",
    "\n",
    "\n",
    "marital_status\tСемейное положение\n",
    "UNM\tХолост/не замужем\n",
    "DIV\tРезведен (а)\n",
    "MAR\tЖенат/замужем\n",
    "WID\tВдовец, вдова\n",
    "CIV\tГражданский брак\n",
    "\n",
    "\n",
    "Gender\tПол\n",
    "F\tЖенский\n",
    "M\tМужской\n",
    "\n",
    "\n",
    "Age\tВозраст\n",
    "credit_sum\tСумма кредита\n",
    "credit_month\tСрок кредитования\n",
    "tariff_id\tНомер предлагаемого тарифа\n",
    "living_region\tРегион проживания\n",
    "monthly_income\tМесячный заработок\n",
    "credit_count\tКоличество кредитов у клиента\n",
    "overdue_credit_count\tКоличество просроченных кредитов клиента\n",
    "score_shk  кредитный скорринг\n",
    "\n",
    "open_account_flg таргет - выбрал ли клиент тинькофф"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mount/neuro-t01-ssd/home/amir/my_env/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import operator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _fillNa(dt, fill_na_train, is_train=True):\n",
    "    \"\"\" 1\n",
    "    Fillig nan values for next columns:\n",
    "    monthly_income, credit_count, overdue_credit_count and living_region.\n",
    "    \"\"\"\n",
    "    if not is_train and (fill_na_train.__len__() == 0):\n",
    "        raise Exception('Run it on train data before!')\n",
    "        \n",
    "    if is_train:\n",
    "        fill_na_train = {}\n",
    "        fill_na_train['monthly_income'] = 30000.0\n",
    "        # но по хорошему надо найти ближайших по работе, \n",
    "        # месту, образованию и возрасту и заменить модой\n",
    "        fill_na_train['credit_count'] = 1.0 \n",
    "        fill_na_train['overdue_credit_count'] = 0.0\n",
    "        fill_na_train['living_region'] = 'ОБЛ МОСКОВСКАЯ'  # 'nan'\n",
    "        \n",
    "    dt['is_nan_monthly_income'] = dt.monthly_income.isnull().astype(int)\n",
    "    dt['is_nan_credit_count'] = dt.credit_count.isnull().astype(int)\n",
    "    dt['is_nan_overdue_credit_count'] = dt.overdue_credit_count.isnull().astype(int)\n",
    "    \n",
    "    dt.fillna(fill_na_train, inplace=True)\n",
    "    return fill_na_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _floatCorrectoin(dt):\n",
    "    \"\"\" 2\n",
    "    \"\"\"\n",
    "    # обработаем запятые и переведем во флоат\n",
    "    dt.credit_sum = dt.credit_sum.apply(\n",
    "        lambda x: x[:-3] + '.' + x[-2:]).astype(float)\n",
    "    dt.score_shk = dt.score_shk.apply(\n",
    "        lambda x: x[0] + '.' + x[2:]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_new_str_features(dt):\n",
    "    \"\"\" 3\n",
    "    Create new features for gender, mar_status,\n",
    "    liv_region and other string columns. \n",
    "    ------------\n",
    "    Return\n",
    "\n",
    "    str_cols : list of str\n",
    "        Columns for encoding and deleting.\n",
    "    \"\"\"\n",
    "\n",
    "    dt['is_female'] = (dt.gender == 'F').astype(int)\n",
    "    \n",
    "    # комбинации education , job_position , gender , living_region\n",
    "    dt['edu_LivReg'] = dt.education + ' ' + dt.living_region\n",
    "    dt['job_LivReg'] = dt.job_position + ' ' + dt.living_region\n",
    "    dt['job_edu'] = dt.job_position + ' ' + dt.education\n",
    "    dt['gender_job'] = dt.gender + ' ' + dt.job_position\n",
    "    dt['gender_LivReg'] = dt.gender + ' ' + dt.living_region\n",
    "    dt['gender_MarSt'] = dt.gender + ' ' + dt.marital_status\n",
    "    dt['LivReg_MarSt'] = dt.gender + ' ' + \\\n",
    "                 dt.living_region + ' ' + dt.marital_status\n",
    "    dt['job_LivReg_marSt_gen'] = dt.gender + ' ' + \\\n",
    "        dt.job_position + ' ' + dt.living_region + \\\n",
    "                            ' ' + dt.marital_status\n",
    "\n",
    "    str_cols = [#'marital_status', 'job_position',\n",
    "                #'education',      'living_region',\n",
    "                #'gender',         \n",
    "                'LivReg_MarSt',\n",
    "                'edu_LivReg',     'job_LivReg',\n",
    "                'job_edu',        'gender_job',\n",
    "                'gender_LivReg',  'gender_MarSt',\n",
    "                'job_LivReg_marSt_gen',\n",
    "                ]\n",
    "    return str_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _counter_encoder(dt, str_cols, counter_Encoders, is_train=True):\n",
    "    \"\"\" 4\n",
    "    Counter Encoder. \n",
    "    Set to the categories from 'str_cols' columns some \n",
    "    numbers - frequencies in train. \n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        counter_Encoders = {col: dt[col].value_counts().to_dict()\n",
    "                            for col in str_cols}\n",
    "    for column in str_cols:\n",
    "        dt[column + '_enc_by_count'] = dt[column].apply(\n",
    "            lambda x: counter_Encoders[column].get(x, 0))\n",
    "        # TODO ровнее бы\n",
    "    return counter_Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _onehot_encoder(dt, str_cols, onehot_Encoders, is_train=True):\n",
    "    \"\"\" 4\n",
    "    Counter Encoder. \n",
    "    Set to the categories from 'str_cols' columns some \n",
    "    numbers - frequencies in train. \n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        onehot_Encoders = {col: dt[col].unique() for col in str_cols}\n",
    "    for column in str_cols:\n",
    "        for value in onehot_Encoders[column]:\n",
    "            dt[column + '_' + value] = (dt[column]==value).astype(int)\n",
    "    return onehot_Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _target_encoder(dt, dt_train, cols_for_encoding, targ_encoders, is_train=True, target_mean=0.176, alpha=1):\n",
    "    \"\"\" 5\n",
    "    Encode cat values by the mean in target.\n",
    "\n",
    "    Params\n",
    "    dt : DataFrame\n",
    "        Data.\n",
    "    cols_for_encoding : list\n",
    "        List of columns for encode.\n",
    "    targ_encoders : dict\n",
    "        Encoder for test dataset.\n",
    "    is_train : bool\n",
    "        Flag for train/test.\n",
    "\n",
    "    ------\n",
    "    Return\n",
    "    targ_encoders : dict\n",
    "        Values of mean target for each category \n",
    "        in columns from cols_for_encoding.\n",
    "        \n",
    "    !!! TODO (mean(y) * K + glob_mean(y) * alpha) / (K + alpha) \n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        targ_encoders = {}\n",
    "        targ_means = {}\n",
    "        for col in cols_for_encoding:\n",
    "            mean_val = dt.groupby(col).open_account_flg.mean()\\\n",
    "                                      .sort_values().index.values\n",
    "            targ_means[col] = mean_val\n",
    "\n",
    "        for col in cols_for_encoding:\n",
    "            targ_encoders[col] = {v: i for i, v in enumerate(targ_means[col])}\n",
    "\n",
    "    for col in cols_for_encoding:\n",
    "        column_length = dt_train[col].shape[0]\n",
    "        all_values, counts = np.unique(dt_train[col].values, return_counts=True)\n",
    "        count_ = {v:count for v, count in zip(all_values, counts)}\n",
    "#         dt[col + '_by_mean_target'] = dt[col].apply(\n",
    "#             lambda colvalue: \n",
    "#             (targ_encoders[col].get(colvalue, target_mean) * count_.get(colvalue,0) + target_mean * alpha)\n",
    "#             /( count_.get(colvalue,0) + alpha))\n",
    "        all_values = all_values[counts > 0.05 * column_length]\n",
    "        dt[col + '_by_mean_target'] = dt[col].apply(\n",
    "                lambda colvalue: targ_encoders[col].get(colvalue, target_mean)\n",
    "                if colvalue in all_values else target_mean)\n",
    "        \n",
    "    return targ_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _new_money_features(dt):\n",
    "    \"\"\" 6\n",
    "    Money features.\n",
    "    \"\"\"\n",
    "\n",
    "    # остаток денег на жизнь человеку\n",
    "    dt['money_residual'] = dt.monthly_income - \\\n",
    "        (dt.credit_sum.values / dt.credit_month.values)\n",
    "    # насколько человек хороший\n",
    "    dt['debts_persent'] = (dt.overdue_credit_count.values +\n",
    "                           1.0) / (dt.credit_count.values + 1.)\n",
    "    # поправка на число кредитов\n",
    "    # остаток денег на жизнь человеку\n",
    "    dt['all_credit_money_residual'] = dt.monthly_income.values / (dt.credit_count.values + 1.) -\\\n",
    "        (dt.credit_sum.values * dt.tariff_id.values / (dt.credit_month.values + 1.))\n",
    "\n",
    "    # поправочка остатка на семейное положение.\n",
    "    dt['_tmp_add'] = (dt['marital_status'] == 'MAR').astype(int) * 1.3\n",
    "    tmp = 1.3 / (1.0 + 0.2 * dt.is_female[dt['marital_status'] == 'DIV'].values)\n",
    "    dt.set_value(dt['marital_status'] == 'DIV', '_tmp_add', tmp)\n",
    "    tmp = 1.0 / (1.0 - 0.2 *  dt.is_female[dt['marital_status'] == 'WID'].values)\n",
    "    dt.set_value(dt['marital_status'] == 'WID', '_tmp_add', tmp)\n",
    "    dt.set_value(dt['marital_status'] == 'UNM', '_tmp_add', 1.0)\n",
    "    dt.set_value(dt['marital_status'] == 'CIV', '_tmp_add', 1.2)\n",
    "\n",
    "    dt['family_money_residual'] = dt[\n",
    "        'money_residual'].values / dt['_tmp_add'].values\n",
    "\n",
    "    dt['strange_money_residual'] = dt['family_money_residual'] / (dt.credit_count + 1.) -\\\n",
    "        (dt.credit_sum.values * dt.tariff_id / (dt.credit_month.values + 1.))\n",
    "\n",
    "    dt['money_residual'] = dt['money_residual']\n",
    "    dt['credit_sum'] = dt['credit_sum'] * dt.tariff_id.values\n",
    "    #dt.drop(['_tmp_add'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# поправочка остатка на местоположение. !!!  это неправильно но можно лучше\n",
    "\n",
    "dt['_tmp_liv_reg'] = dt.living_region.values\n",
    "liv_reg_order = dt.living_region.value_counts().index.values\n",
    "for i, val in enumerate(liv_reg_order):\n",
    "    dt.loc[dt.living_region==val,'_tmp_liv_reg'] = i\n",
    "dt['_tmp_liv_reg'] = dt['_tmp_liv_reg'].astype(int)\n",
    "dt['placed_money_residual'] = dt['money_residual'].values / dt['_tmp_liv_reg'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc_pipline(dt_tr, dt_ts, cv=True):\n",
    "    \"\"\"\n",
    "    Pipline.\n",
    "    \"\"\"\n",
    "\n",
    "    data_train = dt_tr.copy()\n",
    "    data_test = dt_ts.copy()\n",
    "    y_tr = data_train.open_account_flg\n",
    "    if cv:\n",
    "        y_ts = data_test.open_account_flg\n",
    "    #\n",
    "    fill_na_train = _fillNa(data_train, {})\n",
    "    _ = _fillNa(data_test, fill_na_train, is_train=False)\n",
    "    #\n",
    "    _floatCorrectoin(data_train)\n",
    "    _floatCorrectoin(data_test)\n",
    "    #\n",
    "    str_cols = _get_new_str_features(data_train)\n",
    "    _ = _get_new_str_features(data_test)\n",
    "    # > все категории в числа\n",
    "    counter_Encoders = _counter_encoder(data_train, str_cols, {})\n",
    "    _ = _counter_encoder(data_test, str_cols, counter_Encoders, \n",
    "                                                is_train=False)\n",
    "\n",
    "#     one_hot = [ 'marital_status',  'job_position', ]\n",
    "#     onehot_Encoders = _onehot_encoder(data_train, one_hot, {}, is_train=True)\n",
    "#     _ = _onehot_encoder(data_test, one_hot, onehot_Encoders, is_train=False)\n",
    "\n",
    "    #  тут аккуратнее - некоторые приводят к оверфиту ¯\\_(ツ)_/¯ таргет ведь.\n",
    "    cols_for_encoding = [\n",
    "                         'marital_status', \n",
    "                         'living_region',\n",
    "                         'education',   \n",
    "                         'job_position', \n",
    "                         'gender', \n",
    "                         'job_edu',\n",
    "                         #'edu_LivReg',     \n",
    "                         #'job_LivReg', \n",
    "                         #'gender_job',\n",
    "                         #'gender_LivReg',  'gender_MarSt', \n",
    "                         #'LivReg_MarSt', \n",
    "                         #'job_LivReg_marSt_gen',       \n",
    "                         ]\n",
    "\n",
    "    targ_encoders = _target_encoder(data_train, data_train, \n",
    "                cols_for_encoding, {}, is_train=True)\n",
    "    _ = _target_encoder(data_test, data_train, cols_for_encoding,\n",
    "                        targ_encoders, is_train=False)\n",
    "    #\n",
    "    _new_money_features(data_train)\n",
    "    _new_money_features(data_test)\n",
    "    \n",
    "    #\n",
    "    data_train.drop(str_cols, axis=1, inplace=True)\n",
    "    data_test.drop(str_cols, axis=1, inplace=True)\n",
    "    data_train.drop(['open_account_flg'], axis=1, inplace=True)\n",
    "    # если не использовать counter encoder для этих полей, то удалить тк не в str_cols\n",
    "    data_train.drop(['marital_status',  'living_region',\n",
    "                     'education',   'job_position', 'gender'], \n",
    "                    axis=1, inplace=True)\n",
    "    data_test.drop(['marital_status',  'living_region',\n",
    "                     'education',   'job_position', 'gender'], \n",
    "                    axis=1, inplace=True)\n",
    "    #\n",
    "    if cv:\n",
    "        data_test.drop(['open_account_flg'], axis=1, inplace=True)\n",
    "        return data_train, data_test, y_tr, y_ts\n",
    "    else:\n",
    "        return data_train, data_test, y_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('credit_train.csv', encoding='cp1251', sep=';')\n",
    "data_train.drop(['client_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(data_train.open_account_flg.values,\n",
    "                     n_folds=5, shuffle=True, random_state=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 6,\n",
    "    'gamma': 0.1,\n",
    "    'eval_metric': 'auc',\n",
    "    'eta': 0.015,\n",
    "    'booster': 'gbtree',\n",
    "    'seed': 1,\n",
    "    'alpha': 0.1,\n",
    "    'lambda': 0.2,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'subsample': 0.9,\n",
    "    'min_child_weight': 1,\n",
    "    'silent': 1,\n",
    "    'nthread': 9,\n",
    "}\n",
    "\n",
    "num_rounds = 1801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136596, 32)\n",
      "[0]\ttrain-auc:0.704286\teval-auc:0.692771\n",
      "[200]\ttrain-auc:0.771731\teval-auc:0.751099\n",
      "[400]\ttrain-auc:0.787546\teval-auc:0.757835\n",
      "[600]\ttrain-auc:0.798182\teval-auc:0.76067\n",
      "[800]\ttrain-auc:0.806886\teval-auc:0.762102\n",
      "[1000]\ttrain-auc:0.814409\teval-auc:0.762769\n",
      "[1200]\ttrain-auc:0.821711\teval-auc:0.763395\n",
      "[1400]\ttrain-auc:0.828971\teval-auc:0.763733\n",
      "[1600]\ttrain-auc:0.835249\teval-auc:0.76386\n",
      "[1800]\ttrain-auc:0.841481\teval-auc:0.763986\n",
      "(136597, 32)\n",
      "[0]\ttrain-auc:0.704267\teval-auc:0.705055\n",
      "[200]\ttrain-auc:0.770942\teval-auc:0.75852\n",
      "[400]\ttrain-auc:0.786728\teval-auc:0.764698\n",
      "[600]\ttrain-auc:0.797446\teval-auc:0.766991\n",
      "[800]\ttrain-auc:0.806432\teval-auc:0.768033\n",
      "[1000]\ttrain-auc:0.814287\teval-auc:0.76857\n",
      "[1200]\ttrain-auc:0.821831\teval-auc:0.768646\n",
      "[1400]\ttrain-auc:0.828607\teval-auc:0.768727\n",
      "[1600]\ttrain-auc:0.835291\teval-auc:0.768853\n",
      "[1800]\ttrain-auc:0.841597\teval-auc:0.768798\n",
      "(136597, 32)\n",
      "[0]\ttrain-auc:0.705486\teval-auc:0.702512\n",
      "[200]\ttrain-auc:0.771025\teval-auc:0.761057\n",
      "[400]\ttrain-auc:0.78708\teval-auc:0.766886\n",
      "[600]\ttrain-auc:0.797836\teval-auc:0.769337\n",
      "[800]\ttrain-auc:0.806649\teval-auc:0.770494\n",
      "[1000]\ttrain-auc:0.814502\teval-auc:0.770988\n",
      "[1200]\ttrain-auc:0.821857\teval-auc:0.771216\n",
      "[1400]\ttrain-auc:0.828882\teval-auc:0.771386\n",
      "[1600]\ttrain-auc:0.83549\teval-auc:0.771345\n",
      "[1800]\ttrain-auc:0.841723\teval-auc:0.771253\n",
      "(136597, 32)\n",
      "[0]\ttrain-auc:0.705008\teval-auc:0.698792\n",
      "[200]\ttrain-auc:0.770716\teval-auc:0.759567\n",
      "[400]\ttrain-auc:0.786885\teval-auc:0.764519\n",
      "[600]\ttrain-auc:0.798127\teval-auc:0.766715\n",
      "[800]\ttrain-auc:0.807267\teval-auc:0.76769\n",
      "[1000]\ttrain-auc:0.815243\teval-auc:0.768247\n",
      "[1200]\ttrain-auc:0.822198\teval-auc:0.768316\n",
      "[1400]\ttrain-auc:0.828954\teval-auc:0.768343\n",
      "[1600]\ttrain-auc:0.835519\teval-auc:0.768118\n",
      "[1800]\ttrain-auc:0.841784\teval-auc:0.767953\n",
      "(136597, 32)\n",
      "[0]\ttrain-auc:0.704211\teval-auc:0.691958\n",
      "[200]\ttrain-auc:0.772164\teval-auc:0.754207\n",
      "[400]\ttrain-auc:0.787974\teval-auc:0.75957\n",
      "[600]\ttrain-auc:0.798144\teval-auc:0.761503\n",
      "[800]\ttrain-auc:0.806767\teval-auc:0.762692\n",
      "[1000]\ttrain-auc:0.814683\teval-auc:0.763105\n",
      "[1200]\ttrain-auc:0.821713\teval-auc:0.763371\n",
      "[1400]\ttrain-auc:0.828779\teval-auc:0.763624\n",
      "[1600]\ttrain-auc:0.835208\teval-auc:0.763581\n",
      "[1800]\ttrain-auc:0.841442\teval-auc:0.763473\n"
     ]
    }
   ],
   "source": [
    "eval_res = []\n",
    "for tr, ts in cv:\n",
    "    eval_res.append({})\n",
    "    dt_tr, dt_ts, y_tr, y_ts = preproc_pipline(data_train.loc[tr], data_train.loc[ts])\n",
    "    print (dt_tr.shape)\n",
    "    dtrain = xgb.DMatrix(dt_tr.values, label=y_tr, feature_names=dt_tr.columns)\n",
    "    dtest = xgb.DMatrix(dt_ts.values, label=y_ts, feature_names=dt_ts.columns)\n",
    "    \n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "    gbdt = xgb.train(xgb_params, dtrain,\n",
    "                     num_rounds, watchlist,\n",
    "                     early_stopping_rounds=None,\n",
    "                     verbose_eval=200,\n",
    "                     evals_result=eval_res[-1])\n",
    "    # plt.figure()\n",
    "    # xgb.plot_importance(gbdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7670 ± 0.0029\n"
     ]
    }
   ],
   "source": [
    "cv_values = [eval_res[i]['eval']['auc'][-1] for i in range(5)]\n",
    "print (str(np.mean(cv_values))[:6], '±', str(np.std(cv_values))[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light gbm\n",
    "\n",
    "https://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/simple_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-tuning.md\n",
    "\n",
    "lgb_params = { \n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 256,\n",
    "    'max_depth':7, \n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l2':0.1,\n",
    "    'lambda_l1':0.1,\n",
    "    'verbose': 0, \n",
    "    'nthread': 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136596, 32)\n",
      "Start training...\n",
      "[500]\tvalid_0's auc: 0.763051\n",
      "[1000]\tvalid_0's auc: 0.763081\n",
      "[1500]\tvalid_0's auc: 0.763081\n",
      "[2000]\tvalid_0's auc: 0.763081\n",
      "[2500]\tvalid_0's auc: 0.763081\n",
      "[3000]\tvalid_0's auc: 0.763081\n",
      "[3500]\tvalid_0's auc: 0.763081\n",
      "[4000]\tvalid_0's auc: 0.763081\n",
      "Finish first 500 rounds...\n",
      "7th feature name is: 'feature_credit_count'\n",
      "(136597, 32)\n",
      "Start training...\n",
      "[500]\tvalid_0's auc: 0.768939\n",
      "[1000]\tvalid_0's auc: 0.768965\n",
      "[1500]\tvalid_0's auc: 0.768966\n",
      "[2000]\tvalid_0's auc: 0.768966\n",
      "[2500]\tvalid_0's auc: 0.768966\n",
      "[3000]\tvalid_0's auc: 0.768966\n",
      "[3500]\tvalid_0's auc: 0.768966\n",
      "[4000]\tvalid_0's auc: 0.768966\n",
      "Finish first 500 rounds...\n",
      "7th feature name is: 'feature_credit_count'\n",
      "(136597, 32)\n",
      "Start training...\n",
      "[500]\tvalid_0's auc: 0.77023\n",
      "[1000]\tvalid_0's auc: 0.770256\n",
      "[1500]\tvalid_0's auc: 0.770256\n",
      "[2000]\tvalid_0's auc: 0.770256\n",
      "[2500]\tvalid_0's auc: 0.770256\n",
      "[3000]\tvalid_0's auc: 0.770256\n",
      "[3500]\tvalid_0's auc: 0.770256\n",
      "[4000]\tvalid_0's auc: 0.770256\n",
      "Finish first 500 rounds...\n",
      "7th feature name is: 'feature_credit_count'\n",
      "(136597, 32)\n",
      "Start training...\n",
      "[500]\tvalid_0's auc: 0.767874\n",
      "[1000]\tvalid_0's auc: 0.767891\n",
      "[1500]\tvalid_0's auc: 0.767891\n",
      "[2000]\tvalid_0's auc: 0.767891\n",
      "[2500]\tvalid_0's auc: 0.767891\n",
      "[3000]\tvalid_0's auc: 0.767891\n",
      "[3500]\tvalid_0's auc: 0.767891\n",
      "[4000]\tvalid_0's auc: 0.767891\n",
      "Finish first 500 rounds...\n",
      "7th feature name is: 'feature_credit_count'\n",
      "(136597, 32)\n",
      "Start training...\n",
      "[500]\tvalid_0's auc: 0.762348\n",
      "[1000]\tvalid_0's auc: 0.762368\n",
      "[1500]\tvalid_0's auc: 0.762368\n",
      "[2000]\tvalid_0's auc: 0.762368\n",
      "[2500]\tvalid_0's auc: 0.762368\n",
      "[3000]\tvalid_0's auc: 0.762368\n",
      "[3500]\tvalid_0's auc: 0.762368\n",
      "[4000]\tvalid_0's auc: 0.762368\n",
      "Finish first 500 rounds...\n",
      "7th feature name is: 'feature_credit_count'\n"
     ]
    }
   ],
   "source": [
    "eval_res = []\n",
    "for tr, ts in cv:\n",
    "    eval_res.append({})\n",
    "    dt_tr, dt_ts, y_tr, y_ts = preproc_pipline(data_train.loc[tr], data_train.loc[ts])\n",
    "    print (dt_tr.shape)\n",
    "    \n",
    "    lgb_train = lgb.Dataset(dt_tr.values, y_tr, free_raw_data=False)\n",
    "    lgb_eval = lgb.Dataset(dt_ts.values, y_ts, reference=lgb_train, free_raw_data=False)\n",
    "        \n",
    "    # generate a feature name\n",
    "    feature_name = ['feature_' + str(col) for col in dt_tr.columns.values]\n",
    "\n",
    "    print('Start training...')\n",
    "    gbm = lgb.train(lgb_params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=4001,\n",
    "                    verbose_eval=500,\n",
    "                    valid_sets=lgb_eval,  # eval\n",
    "                    learning_rates=lambda iter: 0.1 * (0.99 ** iter),\n",
    "                    feature_name=feature_name,\n",
    "                    evals_result=eval_res[-1])\n",
    "\n",
    "    # check feature name\n",
    "    print('Finish first 500 rounds...')\n",
    "    print('7th feature name is:', repr(lgb_train.feature_name[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7665 ± 0.0031\n"
     ]
    }
   ],
   "source": [
    "cv_values = [eval_res[i]['valid_0']['auc'][-1] for i in range(5)]\n",
    "print (str(np.mean(cv_values))[:6], '±', str(np.std(cv_values))[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### hyperopt for xgboost "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def score(params):\n",
    "    print (\"Training with params : \")\n",
    "    print (params)\n",
    "    num_round = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "    predictions = model.predict(dvalid)\n",
    "    score = roc_auc_score(y_test, predictions)\n",
    "    print (\"\\tScore {0}\\n\\n\".format(score))\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "             'n_estimators' : hp.quniform('n_estimators', 1000, 5000, 10),\n",
    "             'eta' : hp.quniform('eta', 0.015, 0.3, 0.05),\n",
    "             'max_depth' : hp.quniform('max_depth', 3, 9, 1),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0.0, 1, 0.05),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "             'eval_metric': 'auc',\n",
    "             'objective': 'binary:logistic',\n",
    "             'nthread' : 8,\n",
    "             'silent' : 1\n",
    "             }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=250)\n",
    "    print (best)\n",
    "\n",
    "\n",
    "\n",
    "print (\"Splitting data into train and valid ...\\n\\n\")\n",
    "\n",
    "for tr, ts in cv:\n",
    "    pass\n",
    "    break\n",
    "    \n",
    "X_train, X_test, y_train, y_test  = preproc_pipline(data_train.loc[tr], \n",
    "                                                    data_train.loc[ts])\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "#Trials object where the history of search will be stored\n",
    "trials = Trials()\n",
    "\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "trials.results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "trials.trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('credit_train.csv', encoding='cp1251', sep=';')\n",
    "data_test = pd.read_csv('credit_test.csv', encoding='cp1251', sep=';')\n",
    "\n",
    "data_train.drop(['client_id'], axis=1, inplace=True)\n",
    "data_test.drop(['client_id'], axis=1, inplace=True)\n",
    "\n",
    "data_train, data_test, y = preproc_pipline(data_train, data_test, cv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data_train.values, label=y, \n",
    "                     feature_names=data_train.columns)\n",
    "gbdt = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "gbdt.save_model('xgb.model') ### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     importance = gbdt.get_fscore()\n",
    "#     importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "#     df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "#     df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "#     pd.options.display.max_rows = 300\n",
    "#     df.sort(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gbdt = xgb.Booster()  # init model\n",
    "# gbdt.load_model(PATH_FOR_MODEL_DUMPING)  # load model\n",
    "dtest = xgb.DMatrix(data_test.values, feature_names=data_test.columns)\n",
    "ans = gbdt.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_ID_</th>\n",
       "      <th>_VAL_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170747</td>\n",
       "      <td>0.061901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170748</td>\n",
       "      <td>0.120279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170749</td>\n",
       "      <td>0.269779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170750</td>\n",
       "      <td>0.171574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170751</td>\n",
       "      <td>0.101115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _ID_     _VAL_\n",
       "0  170747  0.061901\n",
       "1  170748  0.120279\n",
       "2  170749  0.269779\n",
       "3  170750  0.171574\n",
       "4  170751  0.101115"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = pd.read_csv('credit_test.csv', encoding='cp1251',\n",
    "                  sep=';', usecols=['client_id']).client_id\n",
    "answer = pd.DataFrame({'_ID_': idx, '_VAL_': ans})\n",
    "answer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer.to_csv('answer__xgb' + str(datetime.datetime.now())[5:19] + \\\n",
    "              '__.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train lightgbm & submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('credit_train.csv', encoding='cp1251', sep=';')\n",
    "data_test = pd.read_csv('credit_test.csv', encoding='cp1251', sep=';')\n",
    "\n",
    "data_train.drop(['client_id'], axis=1, inplace=True)\n",
    "data_test.drop(['client_id'], axis=1, inplace=True)\n",
    "\n",
    "data_train, data_test, y = preproc_pipline(data_train, data_test, cv=False)\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset(data_train.values, y)\n",
    "\n",
    "# generate a feature name\n",
    "feature_name = ['feature_' + str(col) for col in data_train.columns.values]\n",
    "\n",
    "print('Start training...')\n",
    "gbm = lgb.train(lgb_params,\n",
    "                lgb_train,\n",
    "                num_boost_round=800,\n",
    "                learning_rates=lambda iter: 0.05 * (0.99 ** iter),\n",
    "                feature_name=feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lgb_test = lgb.Dataset(data_test.values)\n",
    "ans = gbm.predict(data_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_ID_</th>\n",
       "      <th>_VAL_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170747</td>\n",
       "      <td>0.075056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170748</td>\n",
       "      <td>0.146499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170749</td>\n",
       "      <td>0.235342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170750</td>\n",
       "      <td>0.183918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170751</td>\n",
       "      <td>0.101219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _ID_     _VAL_\n",
       "0  170747  0.075056\n",
       "1  170748  0.146499\n",
       "2  170749  0.235342\n",
       "3  170750  0.183918\n",
       "4  170751  0.101219"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = pd.read_csv('credit_test.csv', encoding='cp1251',\n",
    "                  sep=';', usecols=['client_id']).client_id\n",
    "answer = pd.DataFrame({'_ID_': idx, '_VAL_': ans})\n",
    "answer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer.to_csv('answer__lgbm' + str(datetime.datetime.now())[5:19] + \\\n",
    "              '__.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не успел сблендить. забавно даже было бы попробовать lightgbm в действии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
