{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ответом в первой задаче будут два числа. \n",
    "Вам предстоит распарсить объявления и \n",
    "А) узнать сколько на сайте Avito в городе Москве объявлений в категории \"Монеты\" \n",
    "(Хобби и отдых -> Коллекционирование -> Монеты) без указания срока выпуска монеты,  \n",
    "Б) какое соотношение монет выпущенных до 2000 года, к монетам выпущенным после.\n",
    "\n",
    "Допущения, которыми можно руководствоваться при решении задачи: \n",
    "- Если в одном объявлении продают сразу несколько монет, то нужно брать год выпуска первого предложения.\n",
    "- Если указан не точный год, а интервал выпуска монеты, то нужно брать нижнюю границу интервала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from time import sleep\n",
    "import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from tqdm import *\n",
    "import requests\n",
    "from user_agent import generate_user_agent\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://www.avito.ru'\n",
    "MONET_URL = BASE_URL + '/moskva/kollektsionirovanie/monety'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_proxies_are_worked!\n",
      "\n",
      "\n",
      "all_proxies_are_worked_at_avito!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "proxies_list = [{'http':'http://'+i} for i in ['67.149.217.254:10200',\n",
    "                '64.20.74.24:45554','62.37.237.101:8080',\n",
    "                '180.234.206.77:8080',\n",
    "                '78.11.85.13:8080','109.188.81.101:8080',\n",
    "                '139.59.17.113:8080','191.179.147.46:11421',\n",
    "                '111.68.99.42:8080','80.241.219.66:3128',\n",
    "                '201.20.94.106:8080','216.229.120.173:45554',\n",
    "                '116.58.247.31:3128','103.9.115.142:3128',\n",
    "                '82.164.99.193:10200','80.188.79.138:8080',\n",
    "                '36.75.113.224:8080',\n",
    "                '1.20.204.163:8080','97.77.49.151:45554',\n",
    "                '178.54.44.24:8080',\n",
    "                '65.182.136.153:45554', '111.76.129.223:808',\n",
    "                 '203.142.81.205'+':'+'8080', \n",
    "                '42.202.35.185'+':'+'8118', '189.16.249.114'+':'+'8080',\n",
    "                '66.162.122.24'+':'+'8080']]\n",
    "\n",
    "\n",
    "for i in range(len(proxies_list)):\n",
    "    sleep(np.random.random())\n",
    "    assert 200 == requests.get('https://yandex.ru', \n",
    "                               headers={'User-Agent': generate_user_agent()},\n",
    "                               proxies=proxies_list[i]).status_code\n",
    "print ('all_proxies_are_worked!\\n\\n')\n",
    "\n",
    "for i in range(len(proxies_list)):\n",
    "    assert 200 == requests.get('https://avito.ru', \n",
    "                               headers={'User-Agent': generate_user_agent()},\n",
    "                               proxies=proxies_list[i]).status_code\n",
    "print ('all_proxies_are_worked_at_avito!\\n\\n')\n",
    "\n",
    "def get_proxy():\n",
    "    return proxies_list[np.random.randint(0, len(proxies_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_me_date(txt): \n",
    "    m = re.findall('\\D{1}(\\d{3,4})\\D{1}', ' '+txt+' ')\n",
    "    m = np.array(m).astype(int)\n",
    "    return m[(m < 2100) & (m > 600) & (m != 925)].astype(int)\n",
    "\n",
    "assert np.all(np.array([1992, 1993, 1994, 889, 1000, 1995]) == \\\n",
    "            get_me_date('1992 ghbdtn 1993 fsuer 215 руб dv u2194r '+\\\n",
    "                        'trgg 1994 и 889-991 года b tckb njkm4rj 1000 '+\\\n",
    "                        '23452 1995'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "тк обнаружилась такая проблема что при переходе на страницу дальше сотой \n",
    "https://www.avito.ru/moskva/kollektsionirovanie/monety?p=101\n",
    "нас редиректит на первую то можно применить фильтр по цене и ходить по этим страницам. это работает\n",
    "\n",
    "https://www.avito.ru/moskva/kollektsionirovanie/monety?p=17&pmax=1000&view=list\n",
    "https://www.avito.ru/moskva/kollektsionirovanie/monety?p=17&pmin=1001&view=list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = {} # dictionary - URL:title_text\n",
    "all_refs = [] # list of URLs on pages with descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing_pages::  95%|█████████▌| 190/199 [06:01<00:15,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_page_with_num_191 - Stoped\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1,200), desc='parsing_pages:'):\n",
    "    sleep(np.random.random() + 0.92)\n",
    "    req = requests.get(MONET_URL, \n",
    "                   params={'p':i, 'view':'list', 'pmax':1000},\n",
    "                   headers={'User-Agent': generate_user_agent()},\n",
    "                   proxies=get_proxy())\n",
    "    if MONET_URL == req.url:\n",
    "        break\n",
    "    if str(i) not in req.url:\n",
    "        print ('no_page_with_num_'+str(i)+' - Stoped')\n",
    "        break\n",
    "        #raise Exception('no_page_with_num_'+str(i))\n",
    "    if 200 != req.status_code:\n",
    "        print (i, req.status_code)\n",
    "        continue\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    work_item = soup.find_all('a', class_='description-title-link')\n",
    "    for item in work_item:\n",
    "        all_refs.append(item.attrs['href'])\n",
    "        dates[item.attrs['href']] = {'title':item.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "parsing_pages::   0%|          | 0/185 [00:00<?, ?it/s]\u001b[A\n",
      "parsing_pages::   1%|          | 1/185 [00:01<03:52,  1.26s/it]\u001b[A\n",
      "parsing_pages::   1%|          | 2/185 [00:03<04:18,  1.41s/it]\u001b[A\n",
      "parsing_pages::   2%|▏         | 3/185 [00:04<04:43,  1.56s/it]\u001b[A\n",
      "parsing_pages::   2%|▏         | 4/185 [00:06<05:07,  1.70s/it]\u001b[A\n",
      "parsing_pages::  86%|████████▌ | 159/185 [04:30<00:43,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_page_with_num_160 - Stoped\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1,186), desc='parsing_pages:'):\n",
    "    sleep(np.random.random() + 0.72)\n",
    "    req = requests.get(MONET_URL, \n",
    "                       params={'p':i, 'view':'list', 'pmin':1001}, \n",
    "                       headers={'User-Agent': generate_user_agent()},\n",
    "                       proxies=get_proxy())\n",
    "    if MONET_URL == req.url:\n",
    "        break\n",
    "    if str(i) not in req.url:\n",
    "        print ('no_page_with_num_'+str(i)+' - Stoped')\n",
    "        break\n",
    "        #raise Exception('no_page_with_num_'+str(i))\n",
    "    if 200 != req.status_code:\n",
    "        print (req.status_code, i)\n",
    "        continue\n",
    "        \n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    work_item = soup.find_all('a', class_='description-title-link')\n",
    "    for item in work_item:\n",
    "        all_refs.append(item.attrs['href'])\n",
    "        dates[item.attrs['href']] = {'title':item.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_statistics_title(dates):\n",
    "    tmp_title_dates = []\n",
    "    zeros_title = 0\n",
    "    for url in dates:\n",
    "        title_dates = get_me_date(dates[url]['title'])\n",
    "        if len(title_dates) > 0:\n",
    "            tmp_title_dates.append(title_dates[0])\n",
    "        else:\n",
    "            zeros_title += 1\n",
    "    monets = np.array(tmp_title_dates)\n",
    "    ratio_before_2000_to_after = np.sum(monets < 2000) / np.sum(monets >= 2000)\n",
    "    return (zeros_title, ratio_before_2000_to_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15056, 1.5878564857405704)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_statistics_title(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Можно еще уточнить по описанию, но мне влом."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def parse_pages(url):\n",
    "    \"\"\"\n",
    "    Parse pages with description.\n",
    "    \"\"\"\n",
    "    sleep(1.5*np.random.random() + 0.7 + np.random.random() * np.random.random())\n",
    "    req = requests.get(url, proxies=get_proxy())\n",
    "    if 200 != req.status_code or url != req.url:\n",
    "        print ('err', url)\n",
    "        return\n",
    "        \n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    title = soup.find('span', class_ = 'title-info-title-text')\n",
    "    descr = soup.find('div', class_='item-description-text')\n",
    "\n",
    "    new_dates[url] = {'title': title.text}\n",
    "    new_dates[url]['descr'] = descr_dates"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print (len(all_refs))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "new_dates = {}\n",
    "counter = 0\n",
    "for url in all_refs:\n",
    "    parse_pages(BASE_URL+url)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "new_dates = {}\n",
    "res = Parallel(n_jobs=10, backend='threading', verbose=5)(delayed(parse_pages)(BASE_URL+url) for url in all_refs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "len(new_dates)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_statistics_title_and_descr(new_dates):\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
